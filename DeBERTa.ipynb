{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "sample_submission = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/sample_submission.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python courses python courses, python exercise...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the learning point open digital education. a r...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tech news, latest technology, mobiles, laptops...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the best it certification materials in usa | k...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bioland scientific, for your research needs bi...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              target  \\\n",
       "0  python courses python courses, python exercise...  academic interests   \n",
       "1  the learning point open digital education. a r...  academic interests   \n",
       "2  tech news, latest technology, mobiles, laptops...  academic interests   \n",
       "3  the best it certification materials in usa | k...  academic interests   \n",
       "4  bioland scientific, for your research needs bi...  academic interests   \n",
       "\n",
       "   Word Count  \n",
       "0         125  \n",
       "1         147  \n",
       "2         143  \n",
       "3         364  \n",
       "4         176  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# If you want to clear memory allocated by tensors\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  Word Count\n",
      "0  python courses python courses, python exercise...       0         125\n",
      "1  the learning point open digital education. a r...       0         147\n",
      "2  tech news, latest technology, mobiles, laptops...       0         143\n",
      "3  the best it certification materials in usa | k...       0         364\n",
      "4  bioland scientific, for your research needs bi...       0         176\n",
      "Classes: ['academic interests' 'arts and culture' 'automotives'\n",
      " 'books and literature' 'business and finance' 'careers'\n",
      " 'family and relationships' 'food and drinks' 'health' 'healthy living'\n",
      " 'hobbies and interests' 'home and garden' 'movies' 'music and audio'\n",
      " 'news and politics' 'personal finance' 'pets'\n",
      " 'pharmaceuticals, conditions, and symptoms' 'real estate' 'shopping'\n",
      " 'sports' 'style and fashion' 'technology and computing' 'television'\n",
      " 'travel' 'video gaming']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['text'] = train_df['text'].str.lower().str.strip()\n",
    "test_df['text'] = test_df['text'].str.lower().str.strip()\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['text'], train_df['target'], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Check the results of preprocessing and encoding\n",
    "print(train_df.head())\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'lockdown perfect time tollywood stars introspect line up outside profession actor actress began drive back first love music shruti hasaan others like pranitha subhash turned help others tough times various opening samantha akkineni find out foretell horticulture growing bring about said last found something passionate break job starting get tired answering people asked hobby reply represent counterargument job hobby oh baby actress began journey first harvest cabbage microgreens lockdown carry insta explained one need declamatory lawn backyard gardening using space uncommitted domicile initially used window sill bedroom grow microgreens said interested growing require tray cocopeat seeds cool room used bedroom window lets sunlight partly also gave guide fans farsighted takes cum sprout number days tray needs covered use lamp case one enough sunlight room inspired tollywood diva take gardening first piazza pandemic fear able-bodied feed oneself cut back houses said often hear eat healthy grow healthy even simpler takes little time effort since needs us home stay safe think manage let grow together feed god forbid ever another lockdown ones turn tail store panic gradually love growing mature led origination growwithme take exception people share stories have growing fresh vegetables fruits home showtime things along dookudu actress nominated lakshmi manchu rakul preet singh lakshmi manchu girl nivi planted ejaculate leading brand home kit up daughter contain excitement curiosity learn organic fertilizer work mother felicitous join initiative said pandemic taught us great mass healthy living eating food nourishes eubstance feel extremely proud able turn rakul preet singh growing spinach amaranth coriander st. basil excited chance grow solid food said cover girl experience watch seeds pop grow food celebs lot samantha fan young took challenge started growing vegetables like carrots spinach petroselinum crispum basil lettuce tomatoes baby rocket baby bok choy cucumbers celery started taking smashing interest cooking quite insta stories majili actress seen learning basics cooking professional sridevi jasti also friend started basics like taalimpu smoothies chia pudding went create complete meals like tom yum soup tofu brown rice noodles amaranth curry green beans zucchini kale wrap said slowly starting realize much takes put one repast tollywood diva turned greens warrior stop encourages everyone eat clean fresh makes bio enzyme home compost corner twin drum composter reuse recycle neutralize even break tips use waste like banana peels effectively make compost', 'labels': 10, '__index_level_0__': 412155}\n",
      "{'text': 'trafford garden rooms at trafford garden rooms we have the perfect home solution. with our wide range of garden rooms, for all sizes of gardens, we bring to life your dreams for your home. call us today! trafford garden rooms luxury real estate for sale - property agents & brokers - uptown.com  uptown.com luxury real estate for sale ? property agents & brokers ? get access to exclusive properties for sale in nationwide.  get the best agent/broker to help you! realtors michael & anita marchena are the best temecula realtors. realtors michael & anita marchena are the best realtors in temecula california and can help you buy or sell. why not work with the best temecula realtors?', 'labels': 18, '__index_level_0__': 682961}\n",
      "{'text': 'equl offers enzyme assay kits, reagent mixtures, enzymes, glycobiology, amylase test, carbohydrase tablet tests, protease tablet tests, cofactors and stains, soluble chromogenic substrates, insoluble chromogenic substrates, etc brands including: 3m / a.g.scientific / advanced targeting system / advanced biomatrix / agdia / agilent / ampackapak / auvon / aveslab / avonchem / bachem-peninsula / bd / biosb / bioxcell / bioclone / c&b / cadence / californiapeptideresearch / capillarytubes / cbs / chemetrics / chromotek / clodrosome / dako / diatome / divbio / drummond scientific / dumont / dyesol / e&kscientific / ebpi / electronmicroscopy / elisa systems / emsdiasum / encapsulanano / excell / fhc / finesciencetools (fst) / finewire /frontier institute / fuller lab / gene-tools / genevabiotech / glascol / goldbio / harvard / hausser scientific / hawksley / iba / ibl / ideal-tek / iduron / inscopix / ira / isosep / ist / j-kem / kapak / kerafast / kinematica / king precision glass / lumafluor / magle / mattek / mediagnost / medkoo / megazyme / micromod / miltenyi / mybiosource / nacalai / neuro probe / nisco / optical imaging ltd / orbeco / ovenindustries / paperthermometer / parkell / peninsula laboratories / phadebas / phagoburst / plasticsone / pointe scientific / popper / prokazyme / qorpak / quantifoil / radiation alert / randox / roboz surgical instrument / saint-gobain / sakura / scientific instrument service / se / sekisui diagnostics / scientific industries (si) / sigma / sobioda / spectrapor / stanbio / sutter instrument / swant / synaptic system (sysy) / synergel / synthecon / ted pella / teknova / tissue-tek / toronto research chemicals / trinity biotech / v&p scientific / viagen / wako / willco wells bv / world precision instruments (wpi) / worthington biochemical instruments consumables reagents advanced biomatrix randox randox elisa biomedical  biochemical reagents  laboratory supplies  equipment  antibodies  elisa kits  diagnostic reagents  methods of experimental techniques  general analytical instruments  material testing instruments and equipment  used laboratory equipment  instruments and equipment  life sciences  environmental monitoring equipment   measurement  measuring instruments  rotating wall bioreactor  three-dimensional tissue / stem cell culture system; microcapsule'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-xsmall', use_fast=True)\n",
    "\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(\n",
    "    {'text': train_texts, 'labels': train_labels}))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame(\n",
    "    {'text': val_texts, 'labels': val_labels}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': test_df['text']}))\n",
    "\n",
    "# Display first few entries to verify datasets\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets loaded successfully from disk.\n",
      "{'labels': tensor(10), 'input_ids': tensor([     1,  47600,    801,    326,   1941,  59864,   2906,  16669,  37831,\n",
      "           683,    322,    954,   6392,   4435,   6704,   1196,   1168,    396,\n",
      "           362,    472,    755,  12432,  77926,    303,  22669,    690,    334,\n",
      "           845,  28633,  14342,   2698,  42849,   1387,    408,    690,   2770,\n",
      "           631,    847,   1802,  20782,  37431,    266,   1165,   7164,  25514,\n",
      "           433,    321,  77867,  46376,   1479,    861,    314,    357,    437,\n",
      "           505,    491,   5106,   1464,    688,   1392,    350,   4111,   9652,\n",
      "           355,    921,  10169,   4806,   2993,   3118,  55987,    688,  10169,\n",
      "          6359,   1483,   6704,   1196,   1930,    362,   8839,  17069,   4014,\n",
      "          9496,    268,  47600,   1886,  59107,   2996,    311,    389,    718,\n",
      "        100610,  19101,   6680,   7093,  10962,    478,    754, 118135,  63710,\n",
      "          4114,    427,   1775,  41324,   2553,   1618,   4014,   9496,    268,\n",
      "           357,   1408,   1479,   1449,   8844,  62657,  43620,    297,   4837,\n",
      "          1570,    629,    427,   2553,   1775,   4573,   9522,   7725,    327,\n",
      "          1255,   1653,   1790,    659,  44802,   1046,  30550,  40956,    496,\n",
      "           538,   8844,    634,   1911,    380,   6693,    571,    311,    618,\n",
      "          9522,    629,   2689,   1941,  59864,  35291,    413,  10962,    362,\n",
      "         81539,  36011,   2189,    526,    271,  26889,   2975,  14734,   1174,\n",
      "           396,   3099,    357,    611,   1275,   1672,   1582,   1618,   1582,\n",
      "           402,   9237,   1046,    480,    326,   1622,    515,    634,    381,\n",
      "           425,    992,   1295,    428,   2059,    678,   1618,    603,   2975,\n",
      "          5973,  28290,    632,    501,  47600,   1493,    930,   6214,   1106,\n",
      "          8018,   6673,    472,   1479,   6740,   1379,  43645,   1618,   3456,\n",
      "          3831,    413,   4704,    355,    752,   1497,    286,   1479,   1576,\n",
      "          4574,   5947,    425,    553,   1445,    479,    641,    333,  24944,\n",
      "         10584,   6704,   8316,    507,  93802,    642,  21711,   3638,  86459,\n",
      "          1348,   5093,   5716,   1537,    507,  93802,    642,  21711,   1651,\n",
      "          2030,  43026,   8527, 104623,   1249,   1224,    425,   3562,    322,\n",
      "          1816,   2836,   5864,  10084,    799,   3195,  18043,    374,   1346,\n",
      "         83719,  52996,   1646,   4391,    357,  36011,   2999,    381,    426,\n",
      "          2675,   1582,    821,   2342,    645,  54641,    865,  19131,  68728,\n",
      "           551,   1776,   2504,    526,    930,   3638,  86459,   1348,   5093,\n",
      "          5716,   1537,   1479,  14413, 101761,  30057,   8695,    260,  16774,\n",
      "          2199,   1177,   1618,   2176,    645,    357,   1134,   1651,    517,\n",
      "          1314,   4837,   2812,   1618,    645,  45625,    509,  20782,  37431,\n",
      "          2066,    856,    681,   1719,    696,   1479,   4574,    334,  13633,\n",
      "         14413,  50402,  16856,  66337,   8601,   4076,  16774,  14005,   7859,\n",
      "          1483,   9613,   1483,  67947,  91062,  32582,  22746,    696,    787,\n",
      "         27902,    981,   3133,    817,  59107,   1497,  59654,  15972,   6704,\n",
      "           757,   1101,   8185,   3133,    904,  68404,  80156,   4402,  66852,\n",
      "           327,   1156,    696,   8185,    334,  13167,  13191,    358,  16885,\n",
      "         26718,  34918,  17475,    700,    676,    850,   3977,    334,    264,\n",
      "           358,  31431,   6121,  23790,   3258,   4048,  14294, 101761,  14854,\n",
      "          1509,   6223,  23639,  21085,   5815,    357,   3328,   1392,   2544,\n",
      "           400,   1046,    552,    311,    961,  29733,   1941,  59864,  35291,\n",
      "          1387,  11620,  13933,   1001,   8331,    837,   1672,   1347,   1576,\n",
      "           682,   5990,  13348,    425,  16847,   2362,   6153,   8263,  16847,\n",
      "           649,  16075,  14244,  36182,    402,   1464,   2034,    380,   2150,\n",
      "           334,  11578,  37936,   2864,    365,  16847,      2,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "{'labels': tensor(18), 'input_ids': tensor([     1, 111169,   6293,   2058,   2382,    288, 111169,   6293,   2058,\n",
      "          2382,    301,    286,    262,    801,    425,   1327,    260,    275,\n",
      "           316,   1182,    778,    265,   2058,   2382,    261,    270,    305,\n",
      "          3278,    265,   6358,    261,    301,    861,    264,    432,    290,\n",
      "          3968,    270,    290,    425,    260,    660,    381,    561,    300,\n",
      "        111169,   6293,   2058,   2382,   3367,    609,   1978,    270,   1296,\n",
      "           341,    870,   3249,    429,   9542,    341,  76076,    260,    549,\n",
      "         76076,    260,    549,   3367,    609,   1978,    270,   1296,   1102,\n",
      "           870,   3249,    429,   9542,   1102,    350,    739,    264,   3229,\n",
      "          2206,    270,   1296,    267,   7086,    260,    350,    262,    410,\n",
      "          2645,    320,  44580,    264,    408,    274,    300,  48500,  39892,\n",
      "           429,    299,   9189,   9409,  11025,    281,    262,    410,  45051,\n",
      "           473,  53104,  48500,    260,  48500,  39892,    429,    299,   9189,\n",
      "          9409,  11025,    281,    262,    410,  48500,    267,  45051,    473,\n",
      "         53104,  28225,    263,    295,    408,    274,    809,    289,   1642,\n",
      "           260,    579,    298,    374,    275,    262,    410,  45051,    473,\n",
      "         53104,  48500,    302,      2,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-xsmall')\n",
    "\n",
    "train_dataset_path = 'deberta/tokenized_train_dataset'\n",
    "val_dataset_path = 'deberta/tokenized_val_dataset'\n",
    "test_dataset_path = 'deberta/tokenized_test_dataset'\n",
    "\n",
    "if os.path.exists(train_dataset_path) and os.path.exists(val_dataset_path) and os.path.exists(test_dataset_path):\n",
    "    # Load the tokenized datasets\n",
    "    train_dataset = load_from_disk(train_dataset_path)\n",
    "    val_dataset = load_from_disk(val_dataset_path)\n",
    "    test_dataset = load_from_disk(test_dataset_path)\n",
    "    print(\"Tokenized datasets loaded successfully from disk.\")\n",
    "else:\n",
    "    # Define the tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Perform tokenization without multiprocessing\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Set the format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=[\n",
    "                             'input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=[\n",
    "                           'input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=[\n",
    "                            'input_ids', 'attention_mask'])\n",
    "\n",
    "    # Save the tokenized datasets to disk\n",
    "    os.makedirs('deberta', exist_ok=True)\n",
    "    train_dataset.save_to_disk(train_dataset_path)\n",
    "    val_dataset.save_to_disk(val_dataset_path)\n",
    "    test_dataset.save_to_disk(test_dataset_path)\n",
    "    print(\"Tokenized datasets saved successfully to disk.\")\n",
    "\n",
    "# Check the results of tokenization\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2ForSequenceClassification(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 384)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=384, out_features=26, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'microsoft/deberta-v3-xsmall', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "#model.gradient_checkpointing_enable()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set training arguments with mixed precision enabled\n",
    "training_args = TrainingArguments(\n",
    "    dataloader_num_workers=8,\n",
    "    output_dir='./deberta/results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='none', \n",
    "    logging_steps=5000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    eval_steps=2000,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  \n",
    "    report_to=\"none\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "# Display model details\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=384, out_features=26, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier layer\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer.Trainer object at 0x0000018410BF1FD0>\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "\n",
    "# Define a function for computing metrics\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    return {'f1': f1, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "# Verify trainer configuration\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should output True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d2069a8f324b5bb9b50c0da4b93b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.3953, 'train_samples_per_second': 4764453.088, 'train_steps_per_second': 148889.633, 'train_loss': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=58854, training_loss=0.0, metrics={'train_runtime': 0.3953, 'train_samples_per_second': 4764453.088, 'train_steps_per_second': 148889.633, 'total_flos': 1.2411746375635354e+17, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint='./deberta/results/checkpoint-58854')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import joblib\n",
    "# joblib.dump(label_encoder, 'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Decode the predictions back to original labels\n",
    "pred_labels = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# Create the submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': pred_labels\n",
    "})\n",
    "submission_df.to_csv('deberta_submission.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('deberta/saved_model')\n",
    "tokenizer.save_pretrained('deberta/saved_tokenizer')\n",
    "print(\"DeBERTa-v3-xsmall model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "deberta_predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Save predicted probabilities\n",
    "deberta_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(deberta_predictions.predictions), dim=-1).numpy()\n",
    "np.save('deberta_probabilities.npy', deberta_probabilities)\n",
    "\n",
    "# Save the actual predicted labels\n",
    "deberta_pred_labels = np.argmax(deberta_probabilities, axis=1)\n",
    "np.save('deberta_pred_labels.npy', deberta_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153bb09e997f43e0a6d3193066b14067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the training set for deberta\n",
    "deberta_train_predictions = trainer.predict(train_dataset)\n",
    "\n",
    "# Save predicted probabilities for the training set\n",
    "deberta_train_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(deberta_train_predictions.predictions), dim=-1).numpy()\n",
    "np.save('deberta_train_probabilities.npy', deberta_train_probabilities)\n",
    "\n",
    "# Save the actual predicted labels for the training set\n",
    "deberta_train_pred_labels = np.argmax(deberta_train_probabilities, axis=1)\n",
    "np.save('deberta_train_pred_labels.npy', deberta_train_pred_labels)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire training dataset (no splitting)\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Preprocess the text data as done before\n",
    "train_df['text'] = train_df['text'].str.lower().str.strip()\n",
    "\n",
    "# Encode target labels using the same LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Tokenize the entire training dataset using the same tokenizer\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-xsmall')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "\n",
    "# Convert to a Hugging Face dataset and apply tokenization\n",
    "full_train_dataset = Dataset.from_pandas(train_df[['text', 'target']])\n",
    "full_train_dataset = full_train_dataset.map(tokenize_function, batched=True)\n",
    "full_train_dataset.set_format(\n",
    "    type='torch', columns=['input_ids', 'attention_mask', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load your trained DeBERTa model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'deberta/saved_model', num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'microsoft/deberta-v3-xsmall', use_fast=True)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a TrainingArguments object with FP16 enabled and a larger batch size\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.tettt/results',           # Output directory\n",
    "    per_device_eval_batch_size=64,    # Increase this if your GPU has enough memory\n",
    "    fp16=True                         # Enable mixed precision\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with these arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions on the full training set with gradient computation disabled\n",
    "with torch.no_grad():\n",
    "    deberta_full_train_predictions = trainer.predict(full_train_dataset)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "deberta_full_train_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(deberta_full_train_predictions.predictions), dim=-1\n",
    ").numpy()\n",
    "\n",
    "# Save these probabilities\n",
    "np.save('deberta_full_train_probabilities.npy',\n",
    "        deberta_full_train_probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
