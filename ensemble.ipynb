{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for MiniLM\n",
    "minilm_predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Save predicted probabilities\n",
    "minilm_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(minilm_predictions.predictions), dim=-1).numpy()\n",
    "np.save('minilm_probabilities.npy', minilm_probabilities)\n",
    "\n",
    "# Save the actual predicted labels\n",
    "minilm_pred_labels = np.argmax(minilm_probabilities, axis=1)\n",
    "np.save('minilm_pred_labels.npy', minilm_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "sample_submission = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/sample_submission.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Index              target\n",
      "0  Article_0  academic interests\n",
      "1  Article_1             careers\n",
      "2  Article_2              health\n",
      "3  Article_3  academic interests\n",
      "4  Article_4  academic interests\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "electra_probabilities = np.load('electra_probabilities.npy')\n",
    "minilm_probabilities = np.load('minilm_probabilities.npy')\n",
    "nofine_probabilities = np.load('nofine_model_probabilities.npy')\n",
    "deberta_probabilities = np.load('deberta_probabilities.npy')\n",
    "\n",
    "#ensemble_probabilities = (0.1 * electra_probabilities + 0.6 * minilm_probabilities + 0.3 * nofine_probabilities )\n",
    "\n",
    "# Soft voting: Averaging the probabilities from both models\n",
    "ensemble_probabilities = (deberta_probabilities + minilm_probabilities + nofine_probabilities + electra_probabilities) / 4\n",
    "\n",
    "\n",
    "ensemble_pred_labels = np.argmax(ensemble_probabilities, axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df = pd.read_csv('C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "label_encoder.fit(train_df['target'])\n",
    "final_predictions = label_encoder.inverse_transform(ensemble_pred_labels)\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': final_predictions\n",
    "})\n",
    "submission_df.to_csv('ensemble4_submission.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bert_probabilities.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the predicted probabilities from the base models\u001b[39;00m\n\u001b[0;32m      9\u001b[0m electra_probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melectra_probabilities.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m bert_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert_probabilities.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m deberta_probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeberta_probabilities.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m minilm_probabilities \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminilm_probabilities.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bert_probabilities.npy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the predicted probabilities from the base models\n",
    "electra_probabilities = np.load('electra_probabilities.npy')\n",
    "bert_probabilities = np.load('nofine_model_probabilities.npy')\n",
    "deberta_probabilities = np.load('deberta_probabilities.npy')\n",
    "minilm_probabilities = np.load('minilm_probabilities.npy')\n",
    "\n",
    "# Load the original training data to get the true labels\n",
    "train_df = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
    "train_df['Index'] = 'Article_' + train_df.index.astype(str)\n",
    "train_labels = train_df['target']\n",
    "\n",
    "# Ensure all probability arrays have the same length as the training data\n",
    "train_df = train_df.iloc[:len(electra_probabilities)]\n",
    "train_labels = train_labels.iloc[:len(electra_probabilities)]\n",
    "\n",
    "# Combine the predictions from all models into one single array\n",
    "combined_probabilities = np.hstack(\n",
    "    (electra_probabilities, bert_probabilities, deberta_probabilities, minilm_probabilities))\n",
    "\n",
    "# Split into training and validation sets for the meta-learner\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    combined_probabilities, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the meta-model (Logistic Regression in this case)\n",
    "meta_model = LogisticRegression(max_iter=1000, verbose=1)\n",
    "meta_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the meta-model\n",
    "val_predictions = meta_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "val_f1 = f1_score(y_val, val_predictions, average='weighted')\n",
    "\n",
    "print(f\"Validation Accuracy of Stacked Model: {val_accuracy}\")\n",
    "print(f\"Validation F1 Score of Stacked Model: {val_f1}\")\n",
    "\n",
    "# Apply the stacking model to the test set\n",
    "test_df = pd.read_csv('test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Load predicted probabilities for the test set from each base model\n",
    "electra_test_probabilities = np.load('electra_probabilities.npy')\n",
    "bert_test_probabilities = np.load('nofine_model_probabilities.npy')\n",
    "deberta_test_probabilities = np.load('deberta_test_probabilities.npy')\n",
    "minilm_test_probabilities = np.load('minilm_test_probabilities.npy')\n",
    "\n",
    "# Combine test probabilities\n",
    "combined_test_probabilities = np.hstack(\n",
    "    (electra_test_probabilities, bert_test_probabilities, deberta_test_probabilities, minilm_test_probabilities))\n",
    "\n",
    "# Get final predictions from the meta-model\n",
    "final_predictions = meta_model.predict(combined_test_probabilities)\n",
    "\n",
    "# Create the submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': final_predictions\n",
    "})\n",
    "submission_df.to_csv('stacked_ensemble_submission.csv', index=False)\n",
    "\n",
    "print(\"Stacked ensemble submission file generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 137\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStacked ensemble submission file generated successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 111\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    110\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m--> 111\u001b[0m     final_model, label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stacking_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# Load and prepare test data\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_fibe/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 80\u001b[0m, in \u001b[0;36mtrain_stacking_model\u001b[1;34m(X, y, n_splits)\u001b[0m\n\u001b[0;32m     75\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_onehot\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     77\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     78\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 80\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     oof_predictions[val_idx] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# For test set predictions (if available)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# test_pred = model.predict(X_test)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# test_predictions.append(test_pred)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Convert OOF predictions to class labels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Load the predicted probabilities from the base models\n",
    "    electra_probs = np.load('electra_train_probabilities.npy')\n",
    "    bert_probs = np.load('nofine_train_probabilities.npy')\n",
    "    deberta_probs = np.load('deberta_train_probabilities.npy')\n",
    "    minilm_probs = np.load('minilm_train_probabilities.npy')\n",
    "\n",
    "    # Load the original training data to get the true labels\n",
    "    train_df = pd.read_csv('dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "    train_labels = train_df['target']\n",
    "\n",
    "    # Ensure all arrays have the same length\n",
    "    min_length = min(len(electra_probs), len(bert_probs), len(\n",
    "        deberta_probs), len(minilm_probs), len(train_labels))\n",
    "    electra_probs = electra_probs[:min_length]\n",
    "    bert_probs = bert_probs[:min_length]\n",
    "    deberta_probs = deberta_probs[:min_length]\n",
    "    minilm_probs = minilm_probs[:min_length]\n",
    "    train_labels = train_labels[:min_length]\n",
    "\n",
    "    # Combine the predictions from all models\n",
    "    X = np.hstack((electra_probs, bert_probs, deberta_probs, minilm_probs))\n",
    "    y = train_labels\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_model(input_dim, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_shape=(input_dim,), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_stacking_model(X, y, n_splits=5):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_predictions = np.zeros((X.shape[0], y_onehot.shape[1]))\n",
    "    test_predictions = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_encoded), 1):\n",
    "        print(f\"Training fold {fold}\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]\n",
    "\n",
    "        model = create_model(X.shape[1], y_onehot.shape[1])\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        oof_predictions[val_idx] = model.predict(X_val)\n",
    "\n",
    "        # For test set predictions (if available)\n",
    "        # test_pred = model.predict(X_test)\n",
    "        # test_predictions.append(test_pred)\n",
    "\n",
    "    # Convert OOF predictions to class labels\n",
    "    oof_pred_labels = np.argmax(oof_predictions, axis=1)\n",
    "    oof_pred_labels = label_encoder.inverse_transform(oof_pred_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, oof_pred_labels)\n",
    "    f1 = f1_score(y, oof_pred_labels, average='weighted')\n",
    "\n",
    "    print(f\"Out-of-fold Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Out-of-fold F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return model, label_encoder\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    final_model, label_encoder = train_stacking_model(X, y)\n",
    "\n",
    "    # Load and prepare test data\n",
    "    test_df = pd.read_csv('dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "    electra_test = np.load('electra_probabilities.npy')\n",
    "    bert_test = np.load('nofine_model_probabilities.npy')\n",
    "    deberta_test = np.load('deberta_probabilities.npy')\n",
    "    minilm_test = np.load('minilm_probabilities.npy')\n",
    "\n",
    "    X_test = np.hstack((electra_test, bert_test, deberta_test, minilm_test))\n",
    "\n",
    "    # Make predictions on test data\n",
    "    test_predictions = final_model.predict(X_test)\n",
    "    test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "    test_pred_labels = label_encoder.inverse_transform(test_pred_labels)\n",
    "\n",
    "    # Create submission file\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Index': 'Article_' + test_df.index.astype(str),\n",
    "        'target': test_pred_labels\n",
    "    })\n",
    "    submission_df.to_csv('stacked_ensemble_submission.csv', index=False)\n",
    "    print(\"Stacked ensemble submission file generated successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Memory growth set to True for all GPUs\n",
      "Loading data...\n",
      "Data loaded. X shape: (627774, 104), y shape: (627774,)\n",
      "Training stacking model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training fold 1:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1\n",
      "Fold 1: X_train shape: (502219, 104), y_train shape: (502219, 26)\n",
      "Fold 1: X_val shape: (125555, 104), y_val shape: (125555, 26)\n",
      "Model created successfully\n",
      "Starting model fitting...\n",
      "Executing first batch...\n",
      "First batch executed successfully\n",
      "Predicting on validation data...\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Prediction on validation data successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "    1/31389 [..............................] - ETA: 4:02:20 - loss: 4.2244 - accuracy: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0048s). Check your callbacks.\n",
      "31385/31389 [============================>.] - ETA: 0s - loss: 3.2339 - accuracy: 0.0906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31389/31389 [==============================] - 188s 6ms/step - loss: 3.2339 - accuracy: 0.0906 - val_loss: 3.2172 - val_accuracy: 0.0932\n",
      "Epoch 2/100\n",
      "22685/31389 [====================>.........] - ETA: 49s - loss: 3.2173 - accuracy: 0.0934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training fold 1:   0%|          | 0/5 [05:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 214\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStacked ensemble submission file generated successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 187\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining stacking model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 187\u001b[0m final_model, label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stacking_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 150\u001b[0m, in \u001b[0;36mtrain_stacking_model\u001b[1;34m(X, y, n_splits)\u001b[0m\n\u001b[0;32m    147\u001b[0m         model\u001b[38;5;241m.\u001b[39mpredict(X_val[:\u001b[38;5;241m32\u001b[39m])\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction on validation data successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 150\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced batch size\u001b[39;49;00m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtqdm_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Changed to 1 for more detailed output\u001b[39;49;00m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel fitting completed for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU availability and set memory growth\n",
    "print(\"Num GPUs Available: \", len(\n",
    "    tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set to True for all GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs found. Running on CPU.\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # Load the predicted probabilities from the base models\n",
    "    electra_probs = np.load('electra_train_probabilities.npy')\n",
    "    bert_probs = np.load('nofine_train_probabilities.npy')\n",
    "    deberta_probs = np.load('deberta_train_probabilities.npy')\n",
    "    minilm_probs = np.load('minilm_train_probabilities.npy')\n",
    "\n",
    "    # Load the original training data to get the true labels\n",
    "    train_df = pd.read_csv('dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "    train_labels = train_df['target']\n",
    "\n",
    "    # Ensure all arrays have the same length\n",
    "    min_length = min(len(electra_probs), len(bert_probs), len(\n",
    "        deberta_probs), len(minilm_probs), len(train_labels))\n",
    "    electra_probs = electra_probs[:min_length]\n",
    "    bert_probs = bert_probs[:min_length]\n",
    "    deberta_probs = deberta_probs[:min_length]\n",
    "    minilm_probs = minilm_probs[:min_length]\n",
    "    train_labels = train_labels[:min_length]\n",
    "\n",
    "    # Combine the predictions from all models\n",
    "    X = np.hstack((electra_probs, bert_probs, deberta_probs, minilm_probs))\n",
    "    y = train_labels\n",
    "\n",
    "    print(f\"Data loaded. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def create_model(input_dim, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_shape=(input_dim,), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class TqdmProgressCallback(EarlyStopping):\n",
    "    def __init__(self, epochs, verbose=1, **kwargs):\n",
    "        super(TqdmProgressCallback, self).__init__(**kwargs)\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.progbar = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        super().on_train_begin(logs)\n",
    "        if self.verbose:\n",
    "            self.progbar = tqdm(total=self.epochs, desc=\"Epochs\", leave=False)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        if self.verbose:\n",
    "            self.progbar.update(1)\n",
    "            self.progbar.set_postfix({\n",
    "                'loss': f\"{logs['loss']:.4f}\",\n",
    "                'val_loss': f\"{logs['val_loss']:.4f}\"\n",
    "            })\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super().on_train_end(logs)\n",
    "        if self.verbose:\n",
    "            self.progbar.close()\n",
    "\n",
    "\n",
    "def train_stacking_model(X, y, n_splits=5):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_predictions = np.zeros((X.shape[0], y_onehot.shape[1]))\n",
    "\n",
    "    fold_progress = tqdm(enumerate(skf.split(X, y_encoded),\n",
    "                         1), total=n_splits, desc=\"Folds\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in fold_progress:\n",
    "        print(f\"\\nStarting fold {fold}\")\n",
    "        fold_progress.set_description(f\"Training fold {fold}\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y_onehot[train_idx], y_onehot[val_idx]\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold}: X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(\n",
    "            f\"Fold {fold}: X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "        model = create_model(X.shape[1], y_onehot.shape[1])\n",
    "        print(\"Model created successfully\")\n",
    "\n",
    "        tqdm_callback = TqdmProgressCallback(\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            print(\"Starting model fitting...\")\n",
    "            # For the first epoch, we'll do step-by-step execution\n",
    "            if fold == 1:\n",
    "                print(\"Executing first batch...\")\n",
    "                model.train_on_batch(X_train[:32], y_train[:32])\n",
    "                print(\"First batch executed successfully\")\n",
    "\n",
    "                print(\"Predicting on validation data...\")\n",
    "                model.predict(X_val[:32])\n",
    "                print(\"Prediction on validation data successful\")\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=100,\n",
    "                batch_size=16,  # Reduced batch size\n",
    "                callbacks=[tqdm_callback],\n",
    "                verbose=1  # Changed to 1 for more detailed output\n",
    "            )\n",
    "            print(f\"Model fitting completed for fold {fold}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during training: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "        print(f\"Generating OOF predictions for fold {fold}\")\n",
    "        oof_predictions[val_idx] = model.predict(X_val)\n",
    "        print(f\"OOF predictions generated for fold {fold}\")\n",
    "\n",
    "    fold_progress.close()\n",
    "\n",
    "    # Convert OOF predictions to class labels\n",
    "    oof_pred_labels = np.argmax(oof_predictions, axis=1)\n",
    "    oof_pred_labels = label_encoder.inverse_transform(oof_pred_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, oof_pred_labels)\n",
    "    f1 = f1_score(y, oof_pred_labels, average='weighted')\n",
    "\n",
    "    print(f\"Out-of-fold Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Out-of-fold F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return model, label_encoder\n",
    "\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    X, y = load_data()\n",
    "\n",
    "    print(\"Training stacking model...\")\n",
    "    final_model, label_encoder = train_stacking_model(X, y)\n",
    "\n",
    "    print(\"Loading test data...\")\n",
    "    test_df = pd.read_csv('test.csv', encoding='ISO-8859-1')\n",
    "    electra_test = np.load('electra_probabilities.npy')\n",
    "    bert_test = np.load('nofine_model_probabilities.npy')\n",
    "    deberta_test = np.load('deberta_probabilities.npy')\n",
    "    minilm_test = np.load('minilm_probabilities.npy')\n",
    "\n",
    "    X_test = np.hstack((electra_test, bert_test, deberta_test, minilm_test))\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "    print(\"Making predictions on test data...\")\n",
    "    test_predictions = final_model.predict(X_test, verbose=0)\n",
    "    test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "    test_pred_labels = label_encoder.inverse_transform(test_pred_labels)\n",
    "\n",
    "    print(\"Generating submission file...\")\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Index': 'Article_' + test_df.index.astype(str),\n",
    "        'target': test_pred_labels\n",
    "    })\n",
    "    submission_df.to_csv('stacked_ensemble_submission.csv', index=False)\n",
    "    print(\"Stacked ensemble submission file generated successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DeBERTa train probabilities: (627774, 26)\n",
      "Shape of ELECTRA train probabilities: (627774, 26)\n",
      "Shape of NoFine-tuned BERT train probabilities: (627774, 26)\n",
      "Shape of MiniLM train probabilities: (627774, 26)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_meta_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of MiniLM train probabilities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminilm_train_probabilities\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check the shape of the target labels\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of y_meta_train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_meta_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ensure all predicted probabilities have the same number of rows as y_meta_train\u001b[39;00m\n\u001b[0;32m     15\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_meta_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_meta_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Check the shapes of all individual model probabilities\n",
    "print(\n",
    "    f\"Shape of DeBERTa train probabilities: {deberta_train_probabilities.shape}\")\n",
    "print(\n",
    "    f\"Shape of ELECTRA train probabilities: {electra_train_probabilities.shape}\")\n",
    "print(\n",
    "    f\"Shape of NoFine-tuned BERT train probabilities: {nofine_train_probabilities.shape}\")\n",
    "print(\n",
    "    f\"Shape of MiniLM train probabilities: {minilm_train_probabilities.shape}\")\n",
    "\n",
    "# Check the shape of the target labels\n",
    "print(f\"Shape of y_meta_train: {y_meta_train.shape}\")\n",
    "\n",
    "# Ensure all predicted probabilities have the same number of rows as y_meta_train\n",
    "num_samples = len(y_meta_train)\n",
    "\n",
    "# Truncate each model's probability array if necessary\n",
    "deberta_train_probabilities = deberta_train_probabilities[:num_samples]\n",
    "electra_train_probabilities = electra_train_probabilities[:num_samples]\n",
    "nofine_train_probabilities = nofine_train_probabilities[:num_samples]\n",
    "minilm_train_probabilities = minilm_train_probabilities[:num_samples]\n",
    "\n",
    "# Concatenate the adjusted probabilities to form the meta-training set\n",
    "X_meta_train = np.concatenate(\n",
    "    [deberta_train_probabilities, electra_train_probabilities,\n",
    "     nofine_train_probabilities, minilm_train_probabilities],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Verify the shape of X_meta_train again\n",
    "print(f\"Shape of X_meta_train after adjustment: {X_meta_train.shape}\")\n",
    "print(f\"Shape of y_meta_train: {y_meta_train.shape}\")\n",
    "\n",
    "# Now split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_meta_train, y_meta_train, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the predicted probabilities from each model\n",
    "# Adjust the path as needed\n",
    "probs_electra = np.load('electra_probabilities.npy')\n",
    "probs_bert = np.load('nofine_model_probabilities.npy')\n",
    "probs_minilm = np.load('minilm_probabilities.npy')\n",
    "probs_deberta = np.load('deberta_probabilities.npy')  # DeBERTa probabilities\n",
    "\n",
    "# Ensure all probability arrays have the same shape\n",
    "assert probs_electra.shape == probs_bert.shape == probs_minilm.shape == probs_deberta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform soft voting by averaging the predicted probabilities\n",
    "average_probs = (probs_electra + probs_bert + probs_minilm + probs_deberta) / 4\n",
    "\n",
    "# Get the final predicted class labels by selecting the class with the highest average probability\n",
    "final_predictions = np.argmax(average_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your label_encoder was saved, let's load it\n",
    "import joblib\n",
    "# Replace with the correct path if different\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Decode the numerical predictions to the original category labels\n",
    "predicted_labels = label_encoder.inverse_transform(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Index              target\n",
      "0  Article_0  academic interests\n",
      "1  Article_1             careers\n",
      "2  Article_2              health\n",
      "3  Article_3  academic interests\n",
      "4  Article_4  academic interests\n"
     ]
    }
   ],
   "source": [
    "# Load the test file to get the correct index values\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Create the submission DataFrame with correct index and target columns\n",
    "submission_df = pd.DataFrame(\n",
    "    {'Index': test_df['Index'], 'target': predicted_labels})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_df.to_csv('whyyyyyy_submission.csv', index=False)\n",
    "\n",
    "# Display the first few rows to ensure correctness\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder\n",
    "\n",
    "# Load submission files generated by each model\n",
    "deberta_df = pd.read_csv('deberta_submission.csv')\n",
    "electra_df = pd.read_csv('electra_submission.csv')\n",
    "minilm_df = pd.read_csv('mini2_submission.csv')\n",
    "nofine_df = pd.read_csv('nofine_submission.csv')\n",
    "\n",
    "# Assuming 'target' column contains the predictions\n",
    "model_predictions = {\n",
    "    \"deberta\": deberta_df['target'],\n",
    "    \"electra\": electra_df['target'],\n",
    "    \"minilm\": minilm_df['target'],\n",
    "    \"nofine\": nofine_df['target']\n",
    "}\n",
    "\n",
    "# Convert to numerical format for weighted voting\n",
    "label_encoder = LabelEncoder()  # Now LabelEncoder is defined\n",
    "all_predictions = np.array([label_encoder.fit_transform(pred)\n",
    "                           for pred in model_predictions.values()])\n",
    "\n",
    "# Assign weights based on individual model performance\n",
    "weights = np.array([0.4, 0.2, 0.3, 0.1])\n",
    "\n",
    "# Perform weighted voting (multiplying weights by the one-hot encoding of each class prediction)\n",
    "weighted_votes = np.zeros((len(deberta_df), len(label_encoder.classes_)))\n",
    "\n",
    "for i, model_pred in enumerate(all_predictions):\n",
    "    for j, class_index in enumerate(model_pred):\n",
    "        weighted_votes[j, class_index] += weights[i]\n",
    "\n",
    "# Final predictions based on the maximum weighted vote\n",
    "final_predictions_indices = np.argmax(weighted_votes, axis=1)\n",
    "final_predictions_labels = label_encoder.inverse_transform(\n",
    "    final_predictions_indices)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(\n",
    "    {'Index': deberta_df['Index'], 'target': final_predictions_labels})\n",
    "submission_df.to_csv('weighted_voting_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load the predicted probabilities for the training set\n",
    "deberta_train_probabilities = np.load(\n",
    "    'deberta_full_train_probabilities.npy')  # DeBERTa probabilities\n",
    "minilm_train_probabilities = np.load(\n",
    "    'minilm_full_train_probabilities.npy')    # MiniLM probabilities\n",
    "\n",
    "# Load your true labels for the training set\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Combine the probabilities to form the meta-features (X_train for meta-model)\n",
    "X_train_meta = np.hstack((deberta_train_probabilities,\n",
    "                          minilm_train_probabilities))\n",
    "\n",
    "# Use the true labels as the target (y_train for meta-model)\n",
    "y_train_meta = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.22619\n",
      "[1]\tvalidation_0-mlogloss:1.90317\n",
      "[2]\tvalidation_0-mlogloss:1.68073\n",
      "[3]\tvalidation_0-mlogloss:1.51079\n",
      "[4]\tvalidation_0-mlogloss:1.37409\n",
      "[5]\tvalidation_0-mlogloss:1.26058\n",
      "[6]\tvalidation_0-mlogloss:1.16439\n",
      "[7]\tvalidation_0-mlogloss:1.08169\n",
      "[8]\tvalidation_0-mlogloss:1.00982\n",
      "[9]\tvalidation_0-mlogloss:0.94686\n",
      "[10]\tvalidation_0-mlogloss:0.89138\n",
      "[11]\tvalidation_0-mlogloss:0.84227\n",
      "[12]\tvalidation_0-mlogloss:0.79863\n",
      "[13]\tvalidation_0-mlogloss:0.75975\n",
      "[14]\tvalidation_0-mlogloss:0.72500\n",
      "[15]\tvalidation_0-mlogloss:0.69383\n",
      "[16]\tvalidation_0-mlogloss:0.66591\n",
      "[17]\tvalidation_0-mlogloss:0.64082\n",
      "[18]\tvalidation_0-mlogloss:0.61830\n",
      "[19]\tvalidation_0-mlogloss:0.59803\n",
      "[20]\tvalidation_0-mlogloss:0.57975\n",
      "[21]\tvalidation_0-mlogloss:0.56329\n",
      "[22]\tvalidation_0-mlogloss:0.54846\n",
      "[23]\tvalidation_0-mlogloss:0.53503\n",
      "[24]\tvalidation_0-mlogloss:0.52291\n",
      "[25]\tvalidation_0-mlogloss:0.51198\n",
      "[26]\tvalidation_0-mlogloss:0.50208\n",
      "[27]\tvalidation_0-mlogloss:0.49315\n",
      "[28]\tvalidation_0-mlogloss:0.48502\n",
      "[29]\tvalidation_0-mlogloss:0.47772\n",
      "[30]\tvalidation_0-mlogloss:0.47107\n",
      "[31]\tvalidation_0-mlogloss:0.46502\n",
      "[32]\tvalidation_0-mlogloss:0.45957\n",
      "[33]\tvalidation_0-mlogloss:0.45464\n",
      "[34]\tvalidation_0-mlogloss:0.45015\n",
      "[35]\tvalidation_0-mlogloss:0.44606\n",
      "[36]\tvalidation_0-mlogloss:0.44234\n",
      "[37]\tvalidation_0-mlogloss:0.43895\n",
      "[38]\tvalidation_0-mlogloss:0.43588\n",
      "[39]\tvalidation_0-mlogloss:0.43311\n",
      "[40]\tvalidation_0-mlogloss:0.43057\n",
      "[41]\tvalidation_0-mlogloss:0.42827\n",
      "[42]\tvalidation_0-mlogloss:0.42618\n",
      "[43]\tvalidation_0-mlogloss:0.42424\n",
      "[44]\tvalidation_0-mlogloss:0.42249\n",
      "[45]\tvalidation_0-mlogloss:0.42091\n",
      "[46]\tvalidation_0-mlogloss:0.41945\n",
      "[47]\tvalidation_0-mlogloss:0.41813\n",
      "[48]\tvalidation_0-mlogloss:0.41691\n",
      "[49]\tvalidation_0-mlogloss:0.41580\n",
      "[50]\tvalidation_0-mlogloss:0.41479\n",
      "[51]\tvalidation_0-mlogloss:0.41385\n",
      "[52]\tvalidation_0-mlogloss:0.41302\n",
      "[53]\tvalidation_0-mlogloss:0.41225\n",
      "[54]\tvalidation_0-mlogloss:0.41154\n",
      "[55]\tvalidation_0-mlogloss:0.41088\n",
      "[56]\tvalidation_0-mlogloss:0.41029\n",
      "[57]\tvalidation_0-mlogloss:0.40974\n",
      "[58]\tvalidation_0-mlogloss:0.40921\n",
      "[59]\tvalidation_0-mlogloss:0.40874\n",
      "[60]\tvalidation_0-mlogloss:0.40831\n",
      "[61]\tvalidation_0-mlogloss:0.40789\n",
      "[62]\tvalidation_0-mlogloss:0.40753\n",
      "[63]\tvalidation_0-mlogloss:0.40718\n",
      "[64]\tvalidation_0-mlogloss:0.40689\n",
      "[65]\tvalidation_0-mlogloss:0.40658\n",
      "[66]\tvalidation_0-mlogloss:0.40632\n",
      "[67]\tvalidation_0-mlogloss:0.40608\n",
      "[68]\tvalidation_0-mlogloss:0.40586\n",
      "[69]\tvalidation_0-mlogloss:0.40566\n",
      "[70]\tvalidation_0-mlogloss:0.40547\n",
      "[71]\tvalidation_0-mlogloss:0.40528\n",
      "[72]\tvalidation_0-mlogloss:0.40510\n",
      "[73]\tvalidation_0-mlogloss:0.40495\n",
      "[74]\tvalidation_0-mlogloss:0.40480\n",
      "[75]\tvalidation_0-mlogloss:0.40465\n",
      "[76]\tvalidation_0-mlogloss:0.40451\n",
      "[77]\tvalidation_0-mlogloss:0.40438\n",
      "[78]\tvalidation_0-mlogloss:0.40426\n",
      "[79]\tvalidation_0-mlogloss:0.40412\n",
      "[80]\tvalidation_0-mlogloss:0.40400\n",
      "[81]\tvalidation_0-mlogloss:0.40390\n",
      "[82]\tvalidation_0-mlogloss:0.40383\n",
      "[83]\tvalidation_0-mlogloss:0.40377\n",
      "[84]\tvalidation_0-mlogloss:0.40371\n",
      "[85]\tvalidation_0-mlogloss:0.40364\n",
      "[86]\tvalidation_0-mlogloss:0.40357\n",
      "[87]\tvalidation_0-mlogloss:0.40353\n",
      "[88]\tvalidation_0-mlogloss:0.40349\n",
      "[89]\tvalidation_0-mlogloss:0.40344\n",
      "[90]\tvalidation_0-mlogloss:0.40341\n",
      "[91]\tvalidation_0-mlogloss:0.40337\n",
      "[92]\tvalidation_0-mlogloss:0.40333\n",
      "[93]\tvalidation_0-mlogloss:0.40330\n",
      "[94]\tvalidation_0-mlogloss:0.40327\n",
      "[95]\tvalidation_0-mlogloss:0.40324\n",
      "[96]\tvalidation_0-mlogloss:0.40323\n",
      "[97]\tvalidation_0-mlogloss:0.40323\n",
      "[98]\tvalidation_0-mlogloss:0.40320\n",
      "[99]\tvalidation_0-mlogloss:0.40317\n",
      "[100]\tvalidation_0-mlogloss:0.40313\n",
      "[101]\tvalidation_0-mlogloss:0.40313\n",
      "[102]\tvalidation_0-mlogloss:0.40310\n",
      "[103]\tvalidation_0-mlogloss:0.40310\n",
      "[104]\tvalidation_0-mlogloss:0.40307\n",
      "[105]\tvalidation_0-mlogloss:0.40306\n",
      "[106]\tvalidation_0-mlogloss:0.40305\n",
      "[107]\tvalidation_0-mlogloss:0.40307\n",
      "[108]\tvalidation_0-mlogloss:0.40306\n",
      "[109]\tvalidation_0-mlogloss:0.40305\n",
      "[110]\tvalidation_0-mlogloss:0.40305\n",
      "[111]\tvalidation_0-mlogloss:0.40303\n",
      "[112]\tvalidation_0-mlogloss:0.40304\n",
      "[113]\tvalidation_0-mlogloss:0.40304\n",
      "[114]\tvalidation_0-mlogloss:0.40304\n",
      "[115]\tvalidation_0-mlogloss:0.40307\n",
      "[116]\tvalidation_0-mlogloss:0.40309\n",
      "[117]\tvalidation_0-mlogloss:0.40309\n",
      "[118]\tvalidation_0-mlogloss:0.40310\n",
      "[119]\tvalidation_0-mlogloss:0.40311\n",
      "[120]\tvalidation_0-mlogloss:0.40313\n",
      "[121]\tvalidation_0-mlogloss:0.40314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n",
       "              n_jobs=None, num_class=26, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n",
       "              n_jobs=None, num_class=26, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n",
       "              n_jobs=None, num_class=26, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into training and validation sets for meta-classifier training\n",
    "X_meta_train, X_meta_val, y_meta_train, y_meta_val = train_test_split(\n",
    "    X_train_meta, y_train_meta, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize XGBoost Classifier\n",
    "meta_classifier = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # For classification tasks\n",
    "    num_class=len(label_encoder.classes_),  # Number of classes\n",
    "    n_estimators=1000,  # Number of trees\n",
    "    max_depth=6,       # Depth of each tree\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    "    tree_method = \"hist\", \n",
    "    device = \"cuda\"  # If you want to use GPU acceleration\n",
    ")\n",
    "\n",
    "# Train the meta-classifier on the training data\n",
    "meta_classifier.fit(X_meta_train, y_meta_train,\n",
    "                    eval_set=[(X_meta_val, y_meta_val)],\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predicted probabilities for the test set\n",
    "deberta_test_probabilities = np.load('deberta_probabilities.npy')\n",
    "minilm_test_probabilities = np.load('minilm_probabilities.npy')\n",
    "\n",
    "# Combine the test probabilities to form the meta-features (X_test for meta-model)\n",
    "X_test_meta = np.hstack((deberta_test_probabilities,\n",
    "                         minilm_test_probabilities))\n",
    "\n",
    "# Predict using the trained meta-classifier\n",
    "meta_predictions = meta_classifier.predict(X_test_meta)\n",
    "\n",
    "# Convert numeric predictions back to the original labels\n",
    "final_predictions = label_encoder.inverse_transform(meta_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Index              target\n",
      "0  Article_0  academic interests\n",
      "1  Article_1             careers\n",
      "2  Article_2  academic interests\n",
      "3  Article_3  academic interests\n",
      "4  Article_4  academic interests\n"
     ]
    }
   ],
   "source": [
    "# Load the test data to get the index column\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    # Assuming 'Index' column is present in your test CSV\n",
    "    'Index': test_df['Index'],\n",
    "    'target': final_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('meta_classifier_submission2.csv', index=False)\n",
    "\n",
    "# Display the first few rows to ensure correctness\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8831\n",
      "Validation F1 Score: 0.8832\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set\n",
    "val_predictions = meta_classifier.predict(X_meta_val)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_meta_val, val_predictions)\n",
    "f1 = f1_score(y_meta_val, val_predictions, average='weighted')\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 13260\n",
      "[LightGBM] [Info] Number of data points in the train set: 697527, number of used features: 52\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 52 dense feature groups (34.59 MB) transferred to GPU in 0.033337 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -2.478378\n",
      "[LightGBM] [Info] Start training from score -3.275618\n",
      "[LightGBM] [Info] Start training from score -3.215194\n",
      "[LightGBM] [Info] Start training from score -2.847845\n",
      "[LightGBM] [Info] Start training from score -3.213980\n",
      "[LightGBM] [Info] Start training from score -3.130339\n",
      "[LightGBM] [Info] Start training from score -3.181936\n",
      "[LightGBM] [Info] Start training from score -3.393353\n",
      "[LightGBM] [Info] Start training from score -3.604682\n",
      "[LightGBM] [Info] Start training from score -3.115330\n",
      "[LightGBM] [Info] Start training from score -3.371364\n",
      "[LightGBM] [Info] Start training from score -3.411482\n",
      "[LightGBM] [Info] Start training from score -3.466503\n",
      "[LightGBM] [Info] Start training from score -3.520795\n",
      "[LightGBM] [Info] Start training from score -3.139799\n",
      "[LightGBM] [Info] Start training from score -3.496753\n",
      "[LightGBM] [Info] Start training from score -3.363837\n",
      "[LightGBM] [Info] Start training from score -3.266780\n",
      "[LightGBM] [Info] Start training from score -3.375967\n",
      "[LightGBM] [Info] Start training from score -3.162541\n",
      "[LightGBM] [Info] Start training from score -3.347196\n",
      "[LightGBM] [Info] Start training from score -3.175292\n",
      "[LightGBM] [Info] Start training from score -3.515429\n",
      "[LightGBM] [Info] Start training from score -3.576102\n",
      "[LightGBM] [Info] Start training from score -3.470367\n",
      "[LightGBM] [Info] Start training from score -3.414223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m\n\u001b[0;32m      5\u001b[0m lightgbm_meta_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\n\u001b[0;32m      6\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Use 'gpu' if you have a compatible GPU, else remove this line\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the LightGBM model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mlightgbm_meta_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_meta_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_meta_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Predict on validation set\u001b[39;00m\n\u001b[0;32m     22\u001b[0m lightgbm_val_predictions \u001b[38;5;241m=\u001b[39m lightgbm_meta_model\u001b[38;5;241m.\u001b[39mpredict(X_meta_val)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\lightgbm\\sklearn.py:1284\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1282\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1284\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\lightgbm\\sklearn.py:955\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    952\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    953\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\lightgbm\\engine.py:307\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    296\u001b[0m     cb(\n\u001b[0;32m    297\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    298\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m         )\n\u001b[0;32m    305\u001b[0m     )\n\u001b[1;32m--> 307\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\lightgbm\\basic.py:4135\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 4135\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\lightgbm\\basic.py:296\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define the LightGBM model\n",
    "lightgbm_meta_model = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=len(label_encoder.classes_),\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    device='gpu',  # Use 'gpu' if you have a compatible GPU, else remove this line\n",
    ")\n",
    "\n",
    "# Train the LightGBM model\n",
    "lightgbm_meta_model.fit(\n",
    "    X_train_meta, y_train_meta,\n",
    "    eval_set=[(X_meta_val, y_meta_val)],\n",
    ")\n",
    "\n",
    "# Predict on validation set\n",
    "lightgbm_val_predictions = lightgbm_meta_model.predict(X_meta_val)\n",
    "\n",
    "# Evaluate performance\n",
    "lightgbm_accuracy = accuracy_score(y_meta_val, lightgbm_val_predictions)\n",
    "lightgbm_f1 = f1_score(\n",
    "    y_meta_val, lightgbm_val_predictions, average='weighted')\n",
    "print(f\"LightGBM Meta-Model Validation Accuracy: {lightgbm_accuracy:.4f}\")\n",
    "print(f\"LightGBM Meta-Model Validation F1 Score: {lightgbm_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 4476 Total: 6143.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.1869920\ttest: 3.1872050\tbest: 3.1872050 (0)\ttotal: 129ms\tremaining: 2m 8s\n",
      "100:\tlearn: 1.5964124\ttest: 1.6127490\tbest: 1.6127490 (100)\ttotal: 5.42s\tremaining: 48.2s\n",
      "200:\tlearn: 1.1102014\ttest: 1.1335193\tbest: 1.1335193 (200)\ttotal: 10.7s\tremaining: 42.4s\n",
      "300:\tlearn: 0.8636026\ttest: 0.8906784\tbest: 0.8906784 (300)\ttotal: 16s\tremaining: 37.1s\n",
      "400:\tlearn: 0.7130083\ttest: 0.7428167\tbest: 0.7428167 (400)\ttotal: 21.3s\tremaining: 31.8s\n",
      "500:\tlearn: 0.6125032\ttest: 0.6444363\tbest: 0.6444363 (500)\ttotal: 26.7s\tremaining: 26.6s\n",
      "600:\tlearn: 0.5495639\ttest: 0.5831553\tbest: 0.5831553 (600)\ttotal: 32.1s\tremaining: 21.3s\n",
      "700:\tlearn: 0.5062327\ttest: 0.5411540\tbest: 0.5411540 (700)\ttotal: 37.6s\tremaining: 16s\n",
      "800:\tlearn: 0.4771041\ttest: 0.5130277\tbest: 0.5130277 (800)\ttotal: 43.1s\tremaining: 10.7s\n",
      "900:\tlearn: 0.4556245\ttest: 0.4921727\tbest: 0.4921727 (900)\ttotal: 48.6s\tremaining: 5.34s\n",
      "999:\tlearn: 0.4407748\ttest: 0.4775970\tbest: 0.4775970 (999)\ttotal: 54.1s\tremaining: 0us\n",
      "bestTest = 0.4775970097\n",
      "bestIteration = 999\n",
      "CatBoost Meta-Model Validation Accuracy: 0.8822\n",
      "CatBoost Meta-Model Validation F1 Score: 0.8822\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define the CatBoost model\n",
    "catboost_meta_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.01,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='MultiClass',\n",
    "    random_seed=42,\n",
    "    task_type='GPU',  # Use 'GPU' if you have a compatible GPU, else use 'CPU'\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train the CatBoost model\n",
    "catboost_meta_model.fit(X_train_meta, y_train_meta, eval_set=(\n",
    "    X_meta_val, y_meta_val), early_stopping_rounds=50)\n",
    "\n",
    "# Predict on validation set\n",
    "catboost_val_predictions = catboost_meta_model.predict(X_meta_val)\n",
    "# Flatten the predictions array\n",
    "catboost_val_predictions = catboost_val_predictions.flatten()\n",
    "\n",
    "# Evaluate performance\n",
    "catboost_accuracy = accuracy_score(y_meta_val, catboost_val_predictions)\n",
    "catboost_f1 = f1_score(\n",
    "    y_meta_val, catboost_val_predictions, average='weighted')\n",
    "print(f\"CatBoost Meta-Model Validation Accuracy: {catboost_accuracy:.4f}\")\n",
    "print(f\"CatBoost Meta-Model Validation F1 Score: {catboost_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/20], Loss: 0.4990, Validation Loss: 0.4946, Validation Accuracy: 0.8820, Validation F1: 0.8821\n",
      "Epoch [2/20], Loss: 0.1115, Validation Loss: 0.4851, Validation Accuracy: 0.8813, Validation F1: 0.8816\n",
      "Epoch [3/20], Loss: 0.2935, Validation Loss: 0.4717, Validation Accuracy: 0.8818, Validation F1: 0.8819\n",
      "Epoch [4/20], Loss: 0.0496, Validation Loss: 0.4599, Validation Accuracy: 0.8813, Validation F1: 0.8813\n",
      "Epoch [5/20], Loss: 0.1208, Validation Loss: 0.4475, Validation Accuracy: 0.8822, Validation F1: 0.8823\n",
      "Epoch [6/20], Loss: 0.9564, Validation Loss: 0.4441, Validation Accuracy: 0.8808, Validation F1: 0.8808\n",
      "Epoch [7/20], Loss: 0.1694, Validation Loss: 0.4404, Validation Accuracy: 0.8818, Validation F1: 0.8819\n",
      "Epoch [8/20], Loss: 1.3750, Validation Loss: 0.4362, Validation Accuracy: 0.8817, Validation F1: 0.8817\n",
      "Epoch [9/20], Loss: 0.0893, Validation Loss: 0.4354, Validation Accuracy: 0.8818, Validation F1: 0.8819\n",
      "Epoch [10/20], Loss: 0.5197, Validation Loss: 0.4323, Validation Accuracy: 0.8818, Validation F1: 0.8821\n",
      "Epoch [11/20], Loss: 0.1178, Validation Loss: 0.4337, Validation Accuracy: 0.8815, Validation F1: 0.8815\n",
      "Epoch [12/20], Loss: 0.0409, Validation Loss: 0.4298, Validation Accuracy: 0.8814, Validation F1: 0.8814\n",
      "Epoch [13/20], Loss: 0.2248, Validation Loss: 0.4283, Validation Accuracy: 0.8823, Validation F1: 0.8823\n",
      "Epoch [14/20], Loss: 0.0086, Validation Loss: 0.4269, Validation Accuracy: 0.8822, Validation F1: 0.8824\n",
      "Epoch [15/20], Loss: 0.0580, Validation Loss: 0.4262, Validation Accuracy: 0.8815, Validation F1: 0.8815\n",
      "Epoch [16/20], Loss: 0.5531, Validation Loss: 0.4272, Validation Accuracy: 0.8816, Validation F1: 0.8816\n",
      "Epoch [17/20], Loss: 0.0554, Validation Loss: 0.4248, Validation Accuracy: 0.8817, Validation F1: 0.8819\n",
      "Epoch [18/20], Loss: 0.1423, Validation Loss: 0.4268, Validation Accuracy: 0.8811, Validation F1: 0.8811\n",
      "Epoch [19/20], Loss: 0.6402, Validation Loss: 0.4253, Validation Accuracy: 0.8814, Validation F1: 0.8815\n",
      "Epoch [20/20], Loss: 0.3645, Validation Loss: 0.4260, Validation Accuracy: 0.8808, Validation F1: 0.8808\n",
      "Submission file 'nn_meta_submission.csv' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load predicted probabilities from DeBERTa and MiniLM models\n",
    "deberta_train_probabilities = np.load('deberta_full_train_probabilities.npy')\n",
    "minilm_train_probabilities = np.load('minilm_full_train_probabilities.npy')\n",
    "\n",
    "# Combine the probabilities as input features for the meta-model\n",
    "X_train_meta = np.hstack(\n",
    "    (deberta_train_probabilities, minilm_train_probabilities))\n",
    "\n",
    "# Load true labels\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_meta = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_meta, y_train_meta, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "# Define a simple feed-forward neural network\n",
    "\n",
    "\n",
    "class MetaLearnerNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MetaLearnerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)          # Second hidden layer\n",
    "        self.output = nn.Linear(32, num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the neural network model\n",
    "input_size = X_train_tensor.shape[1]  # Number of features (input size)\n",
    "num_classes = len(label_encoder.classes_)  # Number of target classes\n",
    "model = MetaLearnerNN(input_size, num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train_tensor.size(0))\n",
    "\n",
    "    for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_X, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        val_predictions = torch.argmax(val_outputs, dim=1)\n",
    "        val_accuracy = accuracy_score(\n",
    "            y_val_tensor.cpu(), val_predictions.cpu())\n",
    "        val_f1 = f1_score(y_val_tensor.cpu(),\n",
    "                          val_predictions.cpu(), average='weighted')\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "deberta_test_probabilities = np.load('deberta_probabilities.npy')\n",
    "minilm_test_probabilities = np.load('minilm_probabilities.npy')\n",
    "X_test_meta = np.hstack(\n",
    "    (deberta_test_probabilities, minilm_test_probabilities))\n",
    "X_test_tensor = torch.tensor(X_test_meta, dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "final_predictions = label_encoder.inverse_transform(\n",
    "    test_predictions.cpu().numpy())\n",
    "\n",
    "# Create submission file\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': test_df['Index'],\n",
    "    'target': final_predictions\n",
    "})\n",
    "submission_df.to_csv('nn_meta_submission.csv', index=False)\n",
    "print(\"Submission file 'nn_meta_submission.csv' generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
