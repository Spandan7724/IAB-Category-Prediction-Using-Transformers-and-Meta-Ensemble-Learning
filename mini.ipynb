{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/91898/Code/fibe/dataset_fibe/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/91898/Code/fibe/dataset_fibe/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m sample_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/91898/Code/fibe/dataset_fibe/sample_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1036\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1090\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1165\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1380\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n\u001b[0;32m   1381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m   1382\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "sample_submission = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/sample_submission.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python courses python courses, python exercise...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the learning point open digital education. a r...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tech news, latest technology, mobiles, laptops...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the best it certification materials in usa | k...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bioland scientific, for your research needs bi...</td>\n",
       "      <td>academic interests</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              target  \\\n",
       "0  python courses python courses, python exercise...  academic interests   \n",
       "1  the learning point open digital education. a r...  academic interests   \n",
       "2  tech news, latest technology, mobiles, laptops...  academic interests   \n",
       "3  the best it certification materials in usa | k...  academic interests   \n",
       "4  bioland scientific, for your research needs bi...  academic interests   \n",
       "\n",
       "   Word Count  \n",
       "0         125  \n",
       "1         147  \n",
       "2         143  \n",
       "3         364  \n",
       "4         176  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5bda8742674f3c8d1729665250b7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/697527 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the entire training dataset (no splitting)\n",
    "from transformers import miniV2Tokenizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Preprocess the text data as done before\n",
    "train_df['text'] = train_df['text'].str.lower().str.strip()\n",
    "\n",
    "# Encode target labels using the same LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Tokenize the entire training dataset using the same tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "\n",
    "# Convert to a Hugging Face dataset and apply tokenization\n",
    "full_train_dataset = Dataset.from_pandas(train_df[['text', 'target']])\n",
    "full_train_dataset = full_train_dataset.map(tokenize_function, batched=True)\n",
    "full_train_dataset.set_format(\n",
    "    type='torch', columns=['input_ids', 'attention_mask', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088c2cc1606a4ac4bec6ba18dd2e236b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load your trained mini model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'mini/saved_model', num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Create a TrainingArguments object with FP16 enabled and a larger batch size\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.tettt/results',           # Output directory\n",
    "    per_device_eval_batch_size=64,    # Increase this if your GPU has enough memory\n",
    "    fp16=True                         # Enable mixed precision\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with these arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions on the full training set with gradient computation disabled\n",
    "with torch.no_grad():\n",
    "    mini_full_train_predictions = trainer.predict(full_train_dataset)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "mini_full_train_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(mini_full_train_predictions.predictions), dim=-1\n",
    ").numpy()\n",
    "\n",
    "# Save these probabilities\n",
    "np.save('minilm_full_train_probabilities.npy',\n",
    "        mini_full_train_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  Word Count\n",
      "0  python courses python courses, python exercise...       0         125\n",
      "1  the learning point open digital education. a r...       0         147\n",
      "2  tech news, latest technology, mobiles, laptops...       0         143\n",
      "3  the best it certification materials in usa | k...       0         364\n",
      "4  bioland scientific, for your research needs bi...       0         176\n",
      "Classes: ['academic interests' 'arts and culture' 'automotives'\n",
      " 'books and literature' 'business and finance' 'careers'\n",
      " 'family and relationships' 'food and drinks' 'health' 'healthy living'\n",
      " 'hobbies and interests' 'home and garden' 'movies' 'music and audio'\n",
      " 'news and politics' 'personal finance' 'pets'\n",
      " 'pharmaceuticals, conditions, and symptoms' 'real estate' 'shopping'\n",
      " 'sports' 'style and fashion' 'technology and computing' 'television'\n",
      " 'travel' 'video gaming']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['text'] = train_df['text'].str.lower().str.strip()\n",
    "test_df['text'] = test_df['text'].str.lower().str.strip()\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['text'], train_df['target'], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Check the results of preprocessing and encoding\n",
    "print(train_df.head())\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'lockdown perfect time tollywood stars introspect line up outside profession actor actress began drive back first love music shruti hasaan others like pranitha subhash turned help others tough times various opening samantha akkineni find out foretell horticulture growing bring about said last found something passionate break job starting get tired answering people asked hobby reply represent counterargument job hobby oh baby actress began journey first harvest cabbage microgreens lockdown carry insta explained one need declamatory lawn backyard gardening using space uncommitted domicile initially used window sill bedroom grow microgreens said interested growing require tray cocopeat seeds cool room used bedroom window lets sunlight partly also gave guide fans farsighted takes cum sprout number days tray needs covered use lamp case one enough sunlight room inspired tollywood diva take gardening first piazza pandemic fear able-bodied feed oneself cut back houses said often hear eat healthy grow healthy even simpler takes little time effort since needs us home stay safe think manage let grow together feed god forbid ever another lockdown ones turn tail store panic gradually love growing mature led origination growwithme take exception people share stories have growing fresh vegetables fruits home showtime things along dookudu actress nominated lakshmi manchu rakul preet singh lakshmi manchu girl nivi planted ejaculate leading brand home kit up daughter contain excitement curiosity learn organic fertilizer work mother felicitous join initiative said pandemic taught us great mass healthy living eating food nourishes eubstance feel extremely proud able turn rakul preet singh growing spinach amaranth coriander st. basil excited chance grow solid food said cover girl experience watch seeds pop grow food celebs lot samantha fan young took challenge started growing vegetables like carrots spinach petroselinum crispum basil lettuce tomatoes baby rocket baby bok choy cucumbers celery started taking smashing interest cooking quite insta stories majili actress seen learning basics cooking professional sridevi jasti also friend started basics like taalimpu smoothies chia pudding went create complete meals like tom yum soup tofu brown rice noodles amaranth curry green beans zucchini kale wrap said slowly starting realize much takes put one repast tollywood diva turned greens warrior stop encourages everyone eat clean fresh makes bio enzyme home compost corner twin drum composter reuse recycle neutralize even break tips use waste like banana peels effectively make compost', 'labels': 10, '__index_level_0__': 412155}\n",
      "{'text': 'trafford garden rooms at trafford garden rooms we have the perfect home solution. with our wide range of garden rooms, for all sizes of gardens, we bring to life your dreams for your home. call us today! trafford garden rooms luxury real estate for sale - property agents & brokers - uptown.com  uptown.com luxury real estate for sale ? property agents & brokers ? get access to exclusive properties for sale in nationwide.  get the best agent/broker to help you! realtors michael & anita marchena are the best temecula realtors. realtors michael & anita marchena are the best realtors in temecula california and can help you buy or sell. why not work with the best temecula realtors?', 'labels': 18, '__index_level_0__': 682961}\n",
      "{'text': 'equl offers enzyme assay kits, reagent mixtures, enzymes, glycobiology, amylase test, carbohydrase tablet tests, protease tablet tests, cofactors and stains, soluble chromogenic substrates, insoluble chromogenic substrates, etc brands including: 3m / a.g.scientific / advanced targeting system / advanced biomatrix / agdia / agilent / ampackapak / auvon / aveslab / avonchem / bachem-peninsula / bd / biosb / bioxcell / bioclone / c&b / cadence / californiapeptideresearch / capillarytubes / cbs / chemetrics / chromotek / clodrosome / dako / diatome / divbio / drummond scientific / dumont / dyesol / e&kscientific / ebpi / electronmicroscopy / elisa systems / emsdiasum / encapsulanano / excell / fhc / finesciencetools (fst) / finewire /frontier institute / fuller lab / gene-tools / genevabiotech / glascol / goldbio / harvard / hausser scientific / hawksley / iba / ibl / ideal-tek / iduron / inscopix / ira / isosep / ist / j-kem / kapak / kerafast / kinematica / king precision glass / lumafluor / magle / mattek / mediagnost / medkoo / megazyme / micromod / miltenyi / mybiosource / nacalai / neuro probe / nisco / optical imaging ltd / orbeco / ovenindustries / paperthermometer / parkell / peninsula laboratories / phadebas / phagoburst / plasticsone / pointe scientific / popper / prokazyme / qorpak / quantifoil / radiation alert / randox / roboz surgical instrument / saint-gobain / sakura / scientific instrument service / se / sekisui diagnostics / scientific industries (si) / sigma / sobioda / spectrapor / stanbio / sutter instrument / swant / synaptic system (sysy) / synergel / synthecon / ted pella / teknova / tissue-tek / toronto research chemicals / trinity biotech / v&p scientific / viagen / wako / willco wells bv / world precision instruments (wpi) / worthington biochemical instruments consumables reagents advanced biomatrix randox randox elisa biomedical  biochemical reagents  laboratory supplies  equipment  antibodies  elisa kits  diagnostic reagents  methods of experimental techniques  general analytical instruments  material testing instruments and equipment  used laboratory equipment  instruments and equipment  life sciences  environmental monitoring equipment   measurement  measuring instruments  rotating wall bioreactor  three-dimensional tissue / stem cell culture system; microcapsule'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts, 'labels': train_labels}))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts, 'labels': val_labels}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': test_df['text']}))\n",
    "\n",
    "# Display first few entries to verify datasets\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets loaded successfully from disk.\n",
      "{'labels': tensor(10), 'input_ids': tensor([  101,  5843,  7698,  3819,  2051,  9565, 26985,  3340, 17174, 13102,\n",
      "        22471,  2240,  2039,  2648,  9518,  3364,  3883,  2211,  3298,  2067,\n",
      "         2034,  2293,  2189, 14021, 22134,  2072,  2038, 14634,  2500,  2066,\n",
      "        10975,  7088,  8322,  4942, 14949,  2232,  2357,  2393,  2500,  7823,\n",
      "         2335,  2536,  3098, 11415, 17712,  4939, 18595,  2424,  2041, 18921,\n",
      "        23567,  7570, 28228, 14561,  3652,  3288,  2055,  2056,  2197,  2179,\n",
      "         2242, 13459,  3338,  3105,  3225,  2131,  5458, 10739,  2111,  2356,\n",
      "        17792,  7514,  5050,  4675,  2906, 22850,  4765,  3105, 17792,  2821,\n",
      "         3336,  3883,  2211,  4990,  2034, 11203, 28540, 12702, 28637,  3619,\n",
      "         5843,  7698,  4287, 16021,  2696,  4541,  2028,  2342, 11703, 10278,\n",
      "        14049, 10168, 16125, 21529,  2478,  2686,  4895,  9006, 22930,  3064,\n",
      "        14383, 28775,  2571,  3322,  2109,  3332,  9033,  3363,  5010,  4982,\n",
      "        12702, 28637,  3619,  2056,  4699,  3652,  5478, 11851, 25033,  5051,\n",
      "         4017,  8079,  4658,  2282,  2109,  5010,  3332, 11082,  9325,  6576,\n",
      "         2036,  2435,  5009,  4599,  2521, 25807,  2098,  3138, 13988, 11867,\n",
      "        22494,  2102,  2193,  2420, 11851,  3791,  3139,  2224, 10437,  2553,\n",
      "         2028,  2438,  9325,  2282,  4427,  9565, 26985, 25992,  2202, 21529,\n",
      "         2034, 22463,  6090,  3207,  7712,  3571,  2583,  1011, 22549,  5438,\n",
      "        25763,  3013,  2067,  3506,  2056,  2411,  2963,  4521,  7965,  4982,\n",
      "         7965,  2130, 16325,  3138,  2210,  2051,  3947,  2144,  3791,  2149,\n",
      "         2188,  2994,  3647,  2228,  6133,  2292,  4982,  2362,  5438,  2643,\n",
      "        27206,  2412,  2178,  5843,  7698,  3924,  2735,  5725,  3573,  6634,\n",
      "         6360,  2293,  3652,  9677,  2419,  4761,  3370,  4982, 24415,  4168,\n",
      "         2202,  6453,  2111,  3745,  3441,  2031,  3652,  4840, 11546, 10962,\n",
      "         2188, 23811,  2477,  2247, 20160,  5283,  8566,  3883,  4222, 21352,\n",
      "        26650, 10958,  5283,  2140,  3653,  3388,  5960, 21352, 26650,  2611,\n",
      "         9152,  5737,  8461,  1041,  3900, 19879,  2618,  2877,  4435,  2188,\n",
      "         8934,  2039,  2684,  5383,  8277, 10628,  4553,  7554, 10768, 28228,\n",
      "        28863,  2147,  2388, 10768, 10415,  9956,  2271,  3693,  6349,  2056,\n",
      "         6090,  3207,  7712,  4036,  2149,  2307,  3742,  7965,  2542,  5983,\n",
      "         2833,  2053,  9496,  4095,  2229,  7327,  5910, 26897,  2514,  5186,\n",
      "         7098,  2583,  2735, 10958,  5283,  2140,  3653,  3388,  5960,  3652,\n",
      "         6714,  6776, 28599,  3372,  2232,  2522,  6862,  4063,  2358,  1012,\n",
      "        14732,  7568,  3382,  4982,  5024,  2833,  2056,  3104,  2611,  3325,\n",
      "         3422,  8079,  3769,  4982,  2833,  8292,  2571,  5910,  2843, 11415,\n",
      "         5470,  2402,  2165,  4119,  2318,  3652, 11546,  2066, 25659,  2015,\n",
      "         6714,  6776,  9004, 13278,  4115,  2819, 15594,  2819, 14732,  2292,\n",
      "         8525,  3401, 12851,  3336,  7596,  3336,  8945,  2243, 16480,  2100,\n",
      "        12731, 24894, 17198,  8292,  3917,  2100,  2318,  2635, 21105,  3037,\n",
      "         8434,  3243, 16021,  2696,  3441, 16686, 18622,  3883,  2464,  4083,\n",
      "        24078,  8434,  2658,  5185, 24844,  2072, 14855, 16643,  2036,  2767,\n",
      "         2318, 24078,  2066, 11937, 11475,  8737,  2226,  5744,  3111,  9610,\n",
      "         2050, 29593,  2253,  3443,  3143, 12278,  2066,  3419,  9805,  2213,\n",
      "        11350,  2000, 11263,  2829,  5785, 27130, 28599,  3372,  2232, 15478,\n",
      "         2665, 13435, 16950, 25955,  3490, 10556,  2571, 10236,  2056,  3254,\n",
      "         3225,  5382,  2172,  3138,  2404,  2028, 16360, 14083,  9565, 26985,\n",
      "        25992,  2357, 15505,  6750,  2644, 16171,  3071,  4521,  4550,  4840,\n",
      "         3084, 16012,  9007,  2188,  4012, 19894,  3420,  5519,  6943,  4012,\n",
      "        19894,  2121,  2128,  8557, 28667,  2100, 14321,  8699,  4697,  2130,\n",
      "         3338, 10247,  2224,  5949,  2066, 15212, 14113,  2015,  6464,  2191,\n",
      "         4012, 19894,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "{'labels': tensor(18), 'input_ids': tensor([  101, 26894,  3871,  4734,  2012, 26894,  3871,  4734,  2057,  2031,\n",
      "         1996,  3819,  2188,  5576,  1012,  2007,  2256,  2898,  2846,  1997,\n",
      "         3871,  4734,  1010,  2005,  2035, 10826,  1997,  5822,  1010,  2057,\n",
      "         3288,  2000,  2166,  2115,  5544,  2005,  2115,  2188,  1012,  2655,\n",
      "         2149,  2651,   999, 26894,  3871,  4734,  9542,  2613,  3776,  2005,\n",
      "         5096,  1011,  3200,  6074,  1004, 20138,  2015,  1011, 28539,  1012,\n",
      "         4012, 28539,  1012,  4012,  9542,  2613,  3776,  2005,  5096,  1029,\n",
      "         3200,  6074,  1004, 20138,  2015,  1029,  2131,  3229,  2000,  7262,\n",
      "         5144,  2005,  5096,  1999,  9053,  1012,  2131,  1996,  2190,  4005,\n",
      "         1013, 20138,  2000,  2393,  2017,   999,  2613,  6591,  2745,  1004,\n",
      "        12918, 28791,  2532,  2024,  1996,  2190,  8915,  4168, 19879,  2613,\n",
      "         6591,  1012,  2613,  6591,  2745,  1004, 12918, 28791,  2532,  2024,\n",
      "         1996,  2190,  2613,  6591,  1999,  8915,  4168, 19879,  2662,  1998,\n",
      "         2064,  2393,  2017,  4965,  2030,  5271,  1012,  2339,  2025,  2147,\n",
      "         2007,  1996,  2190,  8915,  4168, 19879,  2613,  6591,  1029,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "\n",
    "train_dataset_path = 'mini/tokenized_train_dataset'\n",
    "val_dataset_path = 'mini/tokenized_val_dataset'\n",
    "test_dataset_path = 'mini/tokenized_test_dataset'\n",
    "\n",
    "if os.path.exists(train_dataset_path) and os.path.exists(val_dataset_path) and os.path.exists(test_dataset_path):\n",
    "    # Load the tokenized datasets\n",
    "    train_dataset = load_from_disk(train_dataset_path)\n",
    "    val_dataset = load_from_disk(val_dataset_path)\n",
    "    test_dataset = load_from_disk(test_dataset_path)\n",
    "    print(\"Tokenized datasets loaded successfully from disk.\")\n",
    "else:\n",
    "    # Define the tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Perform tokenization without multiprocessing\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Set the format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=[\n",
    "                             'input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=[\n",
    "                           'input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=[\n",
    "                            'input_ids', 'attention_mask'])\n",
    "\n",
    "    # Save the tokenized datasets to disk\n",
    "    os.makedirs('mini', exist_ok=True)\n",
    "    train_dataset.save_to_disk(train_dataset_path)\n",
    "    val_dataset.save_to_disk(val_dataset_path)\n",
    "    test_dataset.save_to_disk(test_dataset_path)\n",
    "    print(\"Tokenized datasets saved successfully to disk.\")\n",
    "\n",
    "# Check the results of tokenization\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 384)\n",
      "      (token_type_embeddings): Embedding(2, 384)\n",
      "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=384, out_features=26, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('microsoft/MiniLM-L12-H384-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set training arguments with mixed precision enabled\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='none',\n",
    "    logging_steps=5000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    report_to=\"none\",\n",
    "    resume_from_checkpoint=True \n",
    ")\n",
    "\n",
    "# Display model details\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer.Trainer object at 0x000001DB805C9460>\n"
     ]
    }
   ],
   "source": [
    "# Define a function for computing metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    return {'f1': f1, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "# Verify trainer configuration\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should output True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tlogging_steps: 5000 (from args) != 500 (from trainer_state.json)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0858f81f5a88429aa01d1a93dca79c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4518, 'grad_norm': 5.853095054626465, 'learning_rate': 1.4609071053170434e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4329, 'grad_norm': 4.974795341491699, 'learning_rate': 1.4396201624462493e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd58e5036a84c71b49dffff8f1ea42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.513631284236908, 'eval_f1': 0.86023275743773, 'eval_accuracy': 0.8601780568577696, 'eval_runtime': 179.9693, 'eval_samples_per_second': 387.583, 'eval_steps_per_second': 6.057, 'epoch': 2.14}\n",
      "{'loss': 0.4544, 'grad_norm': 11.701143264770508, 'learning_rate': 1.4182905603713056e-05, 'epoch': 2.15}\n",
      "{'loss': 0.4325, 'grad_norm': 4.111960411071777, 'learning_rate': 1.3969609582963621e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb24a29cd9464645a2d30000b7a59bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5057610869407654, 'eval_f1': 0.861198629923062, 'eval_accuracy': 0.8610382349146273, 'eval_runtime': 182.3914, 'eval_samples_per_second': 382.436, 'eval_steps_per_second': 5.976, 'epoch': 2.17}\n",
      "{'loss': 0.426, 'grad_norm': 6.57260274887085, 'learning_rate': 1.3756313562214185e-05, 'epoch': 2.18}\n",
      "{'loss': 0.406, 'grad_norm': 4.950322151184082, 'learning_rate': 1.3543017541464748e-05, 'epoch': 2.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c94948b6864e08a3db2c0ce78853ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5197662711143494, 'eval_f1': 0.8594784467376767, 'eval_accuracy': 0.8594325692084929, 'eval_runtime': 183.1527, 'eval_samples_per_second': 380.846, 'eval_steps_per_second': 5.951, 'epoch': 2.19}\n",
      "{'loss': 0.411, 'grad_norm': 9.827994346618652, 'learning_rate': 1.332972152071531e-05, 'epoch': 2.2}\n",
      "{'loss': 0.4428, 'grad_norm': 9.174023628234863, 'learning_rate': 1.3116425499965873e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103ee21c04b544f8bc886b360610426b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5094216465950012, 'eval_f1': 0.8610132360191208, 'eval_accuracy': 0.8609952260117845, 'eval_runtime': 182.8571, 'eval_samples_per_second': 381.462, 'eval_steps_per_second': 5.961, 'epoch': 2.22}\n",
      "{'loss': 0.4149, 'grad_norm': 20.606487274169922, 'learning_rate': 1.2903556071257935e-05, 'epoch': 2.23}\n",
      "{'loss': 0.44, 'grad_norm': 6.644088268280029, 'learning_rate': 1.2690260050508498e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033fe47338cb48d4ba114fac911e5de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5002254843711853, 'eval_f1': 0.8615006775401592, 'eval_accuracy': 0.861167261623156, 'eval_runtime': 183.9291, 'eval_samples_per_second': 379.239, 'eval_steps_per_second': 5.926, 'epoch': 2.24}\n",
      "{'train_runtime': 1739.4298, 'train_samples_per_second': 1082.724, 'train_steps_per_second': 67.67, 'train_loss': 0.024501816662875087, 'epoch': 2.24}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=88000, training_loss=0.024501816662875087, metrics={'train_runtime': 1739.4298, 'train_samples_per_second': 1082.724, 'train_steps_per_second': 67.67, 'total_flos': 9.3007825779499e+16, 'train_loss': 0.024501816662875087, 'epoch': 2.24283820980732})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint='./results/checkpoint-83000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6c3b503c744e91ac8fa532b07b21b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Article_0</td>\n",
       "      <td>technology and computing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Article_1</td>\n",
       "      <td>academic interests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Article_2</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Article_3</td>\n",
       "      <td>academic interests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Article_4</td>\n",
       "      <td>academic interests</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index                    target\n",
       "0  Article_0  technology and computing\n",
       "1  Article_1        academic interests\n",
       "2  Article_2                    health\n",
       "3  Article_3        academic interests\n",
       "4  Article_4        academic interests"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Decode the predictions back to original labels\n",
    "pred_labels = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# Create the submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': pred_labels\n",
    "})\n",
    "submission_df.to_csv('mini2_submission.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('mini/saved_model')\n",
    "tokenizer.save_pretrained('mini/saved_tokenizer')\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm \n",
    "\n",
    "# # Load your data (adjust the path to your dataset)\n",
    "# train_df = pd.read_csv(\n",
    "#     'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "# test_df = pd.read_csv(\n",
    "#     'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# # Initialize the tokenizer (using the same model you are fine-tuning)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('microsoft/MiniLM-L12-H384-uncased')\n",
    "\n",
    "# # Combine text data from train and test for comprehensive analysis\n",
    "# all_texts = pd.concat([train_df['text'], test_df['text']], ignore_index=True)\n",
    "\n",
    "# # Add tqdm progress bar to the loop\n",
    "# lengths = []\n",
    "# for text in tqdm(all_texts, desc=\"Calculating average max_length\", unit=\"text\"):\n",
    "#     length = len(tokenizer.encode(text, truncation=False))\n",
    "#     lengths.append(length)\n",
    "\n",
    "# # Calculate average max_length\n",
    "# average_length = sum(lengths) / len(lengths)\n",
    "# max_length = max(lengths)\n",
    "\n",
    "# print(f\"Average max_length: {average_length}\")\n",
    "# print(f\"Max sequence length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2d9bd59a2e47f8861a3f979da01966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set for MiniLM\n",
    "minilm_predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Save predicted probabilities\n",
    "minilm_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(minilm_predictions.predictions), dim=-1).numpy()\n",
    "np.save('minilm_probabilities.npy', minilm_probabilities)\n",
    "\n",
    "# Save the actual predicted labels\n",
    "minilm_pred_labels = np.argmax(minilm_probabilities, axis=1)\n",
    "np.save('minilm_pred_labels.npy', minilm_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae97576f23d449e8209400abb95c5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the training set for MiniLM\n",
    "minilm_train_predictions = trainer.predict(train_dataset)\n",
    "\n",
    "# Save predicted probabilities for the training set\n",
    "minilm_train_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(minilm_train_predictions.predictions), dim=-1).numpy()\n",
    "np.save('minilm_train_probabilities.npy', minilm_train_probabilities)\n",
    "\n",
    "# Save the actual predicted labels for the training set\n",
    "minilm_train_pred_labels = np.argmax(minilm_train_probabilities, axis=1)\n",
    "np.save('minilm_train_pred_labels.npy', minilm_train_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataset: 627774\n",
      "Length of train_df: 627774\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Length of train_df: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:len(train_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (564996, 26), y_train shape: (564996,)\n",
      "X_val shape: (62778, 26), y_val shape: (62778,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the saved probabilities from MiniLM\n",
    "minilm_probabilities = np.load('minilm_train_probabilities.npy')\n",
    "\n",
    "# Reload and align the train_df to match train_dataset\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "train_df = train_df.iloc[:len(minilm_probabilities)]\n",
    "\n",
    "# Re-encode the actual target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    minilm_probabilities, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:10:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:3.25298\n",
      "[1]\teval-mlogloss:3.24860\n",
      "[2]\teval-mlogloss:3.24478\n",
      "[3]\teval-mlogloss:3.24156\n",
      "[4]\teval-mlogloss:3.23876\n",
      "[5]\teval-mlogloss:3.23636\n",
      "[6]\teval-mlogloss:3.23427\n",
      "[7]\teval-mlogloss:3.23251\n",
      "[8]\teval-mlogloss:3.23091\n",
      "[9]\teval-mlogloss:3.22949\n",
      "[10]\teval-mlogloss:3.22827\n",
      "[11]\teval-mlogloss:3.22722\n",
      "[12]\teval-mlogloss:3.22632\n",
      "[13]\teval-mlogloss:3.22567\n",
      "[14]\teval-mlogloss:3.22498\n",
      "[15]\teval-mlogloss:3.22439\n",
      "[16]\teval-mlogloss:3.22392\n",
      "[17]\teval-mlogloss:3.22355\n",
      "[18]\teval-mlogloss:3.22319\n",
      "[19]\teval-mlogloss:3.22289\n",
      "[20]\teval-mlogloss:3.22267\n",
      "[21]\teval-mlogloss:3.22243\n",
      "[22]\teval-mlogloss:3.22233\n",
      "[23]\teval-mlogloss:3.22217\n",
      "[24]\teval-mlogloss:3.22211\n",
      "[25]\teval-mlogloss:3.22201\n",
      "[26]\teval-mlogloss:3.22193\n",
      "[27]\teval-mlogloss:3.22184\n",
      "[28]\teval-mlogloss:3.22190\n",
      "[29]\teval-mlogloss:3.22188\n",
      "[30]\teval-mlogloss:3.22192\n",
      "[31]\teval-mlogloss:3.22187\n",
      "[32]\teval-mlogloss:3.22188\n",
      "[33]\teval-mlogloss:3.22201\n",
      "[34]\teval-mlogloss:3.22212\n",
      "[35]\teval-mlogloss:3.22219\n",
      "[36]\teval-mlogloss:3.22238\n",
      "[37]\teval-mlogloss:3.22248\n",
      "[38]\teval-mlogloss:3.22263\n",
      "[39]\teval-mlogloss:3.22280\n",
      "[40]\teval-mlogloss:3.22289\n",
      "[41]\teval-mlogloss:3.22294\n",
      "[42]\teval-mlogloss:3.22306\n",
      "[43]\teval-mlogloss:3.22317\n",
      "[44]\teval-mlogloss:3.22327\n",
      "[45]\teval-mlogloss:3.22342\n",
      "[46]\teval-mlogloss:3.22361\n",
      "[47]\teval-mlogloss:3.22368\n",
      "[48]\teval-mlogloss:3.22380\n",
      "[49]\teval-mlogloss:3.22396\n",
      "[50]\teval-mlogloss:3.22418\n",
      "[51]\teval-mlogloss:3.22441\n",
      "[52]\teval-mlogloss:3.22452\n",
      "[53]\teval-mlogloss:3.22463\n",
      "[54]\teval-mlogloss:3.22481\n",
      "[55]\teval-mlogloss:3.22486\n",
      "[56]\teval-mlogloss:3.22498\n",
      "[57]\teval-mlogloss:3.22520\n",
      "[58]\teval-mlogloss:3.22523\n",
      "[59]\teval-mlogloss:3.22539\n",
      "[60]\teval-mlogloss:3.22553\n",
      "[61]\teval-mlogloss:3.22575\n",
      "[62]\teval-mlogloss:3.22590\n",
      "[63]\teval-mlogloss:3.22607\n",
      "[64]\teval-mlogloss:3.22629\n",
      "[65]\teval-mlogloss:3.22643\n",
      "[66]\teval-mlogloss:3.22658\n",
      "[67]\teval-mlogloss:3.22672\n",
      "[68]\teval-mlogloss:3.22696\n",
      "[69]\teval-mlogloss:3.22717\n",
      "[70]\teval-mlogloss:3.22736\n",
      "[71]\teval-mlogloss:3.22756\n",
      "[72]\teval-mlogloss:3.22775\n",
      "[73]\teval-mlogloss:3.22789\n",
      "[74]\teval-mlogloss:3.22804\n",
      "[75]\teval-mlogloss:3.22820\n",
      "[76]\teval-mlogloss:3.22834\n",
      "[77]\teval-mlogloss:3.22845\n",
      "[78]\teval-mlogloss:3.22864\n",
      "[79]\teval-mlogloss:3.22884\n",
      "[80]\teval-mlogloss:3.22903\n",
      "[81]\teval-mlogloss:3.22919\n",
      "[82]\teval-mlogloss:3.22935\n",
      "[83]\teval-mlogloss:3.22950\n",
      "[84]\teval-mlogloss:3.22967\n",
      "[85]\teval-mlogloss:3.22985\n",
      "[86]\teval-mlogloss:3.23011\n",
      "[87]\teval-mlogloss:3.23022\n",
      "[88]\teval-mlogloss:3.23042\n",
      "[89]\teval-mlogloss:3.23056\n",
      "[90]\teval-mlogloss:3.23072\n",
      "[91]\teval-mlogloss:3.23081\n",
      "[92]\teval-mlogloss:3.23103\n",
      "[93]\teval-mlogloss:3.23120\n",
      "[94]\teval-mlogloss:3.23130\n",
      "[95]\teval-mlogloss:3.23145\n",
      "[96]\teval-mlogloss:3.23165\n",
      "[97]\teval-mlogloss:3.23184\n",
      "[98]\teval-mlogloss:3.23207\n",
      "[99]\teval-mlogloss:3.23227\n",
      "[100]\teval-mlogloss:3.23244\n",
      "[101]\teval-mlogloss:3.23258\n",
      "[102]\teval-mlogloss:3.23273\n",
      "[103]\teval-mlogloss:3.23294\n",
      "[104]\teval-mlogloss:3.23305\n",
      "[105]\teval-mlogloss:3.23315\n",
      "[106]\teval-mlogloss:3.23334\n",
      "[107]\teval-mlogloss:3.23350\n",
      "[108]\teval-mlogloss:3.23363\n",
      "[109]\teval-mlogloss:3.23373\n",
      "[110]\teval-mlogloss:3.23392\n",
      "[111]\teval-mlogloss:3.23406\n",
      "[112]\teval-mlogloss:3.23420\n",
      "[113]\teval-mlogloss:3.23430\n",
      "[114]\teval-mlogloss:3.23447\n",
      "[115]\teval-mlogloss:3.23470\n",
      "[116]\teval-mlogloss:3.23484\n",
      "[117]\teval-mlogloss:3.23500\n",
      "[118]\teval-mlogloss:3.23519\n",
      "[119]\teval-mlogloss:3.23530\n",
      "[120]\teval-mlogloss:3.23541\n",
      "[121]\teval-mlogloss:3.23560\n",
      "[122]\teval-mlogloss:3.23587\n",
      "[123]\teval-mlogloss:3.23603\n",
      "[124]\teval-mlogloss:3.23615\n",
      "[125]\teval-mlogloss:3.23639\n",
      "[126]\teval-mlogloss:3.23652\n",
      "[127]\teval-mlogloss:3.23663\n",
      "[128]\teval-mlogloss:3.23674\n",
      "[129]\teval-mlogloss:3.23683\n",
      "[130]\teval-mlogloss:3.23695\n",
      "[131]\teval-mlogloss:3.23717\n",
      "[132]\teval-mlogloss:3.23729\n",
      "[133]\teval-mlogloss:3.23754\n",
      "[134]\teval-mlogloss:3.23779\n",
      "[135]\teval-mlogloss:3.23796\n",
      "[136]\teval-mlogloss:3.23818\n",
      "[137]\teval-mlogloss:3.23828\n",
      "[138]\teval-mlogloss:3.23838\n",
      "[139]\teval-mlogloss:3.23857\n",
      "[140]\teval-mlogloss:3.23877\n",
      "[141]\teval-mlogloss:3.23900\n",
      "[142]\teval-mlogloss:3.23920\n",
      "[143]\teval-mlogloss:3.23934\n",
      "[144]\teval-mlogloss:3.23957\n",
      "[145]\teval-mlogloss:3.23975\n",
      "[146]\teval-mlogloss:3.23994\n",
      "[147]\teval-mlogloss:3.24011\n",
      "[148]\teval-mlogloss:3.24028\n",
      "[149]\teval-mlogloss:3.24042\n",
      "[150]\teval-mlogloss:3.24056\n",
      "[151]\teval-mlogloss:3.24071\n",
      "[152]\teval-mlogloss:3.24089\n",
      "[153]\teval-mlogloss:3.24098\n",
      "[154]\teval-mlogloss:3.24110\n",
      "[155]\teval-mlogloss:3.24130\n",
      "[156]\teval-mlogloss:3.24144\n",
      "[157]\teval-mlogloss:3.24161\n",
      "[158]\teval-mlogloss:3.24183\n",
      "[159]\teval-mlogloss:3.24194\n",
      "[160]\teval-mlogloss:3.24213\n",
      "[161]\teval-mlogloss:3.24227\n",
      "[162]\teval-mlogloss:3.24245\n",
      "[163]\teval-mlogloss:3.24255\n",
      "[164]\teval-mlogloss:3.24271\n",
      "[165]\teval-mlogloss:3.24289\n",
      "[166]\teval-mlogloss:3.24308\n",
      "[167]\teval-mlogloss:3.24322\n",
      "[168]\teval-mlogloss:3.24336\n",
      "[169]\teval-mlogloss:3.24339\n",
      "[170]\teval-mlogloss:3.24357\n",
      "[171]\teval-mlogloss:3.24382\n",
      "[172]\teval-mlogloss:3.24395\n",
      "[173]\teval-mlogloss:3.24412\n",
      "[174]\teval-mlogloss:3.24426\n",
      "[175]\teval-mlogloss:3.24440\n",
      "[176]\teval-mlogloss:3.24459\n",
      "[177]\teval-mlogloss:3.24478\n",
      "[178]\teval-mlogloss:3.24496\n",
      "[179]\teval-mlogloss:3.24516\n",
      "[180]\teval-mlogloss:3.24524\n",
      "[181]\teval-mlogloss:3.24539\n",
      "[182]\teval-mlogloss:3.24554\n",
      "[183]\teval-mlogloss:3.24573\n",
      "[184]\teval-mlogloss:3.24591\n",
      "[185]\teval-mlogloss:3.24600\n",
      "[186]\teval-mlogloss:3.24609\n",
      "[187]\teval-mlogloss:3.24624\n",
      "[188]\teval-mlogloss:3.24634\n",
      "[189]\teval-mlogloss:3.24647\n",
      "[190]\teval-mlogloss:3.24664\n",
      "[191]\teval-mlogloss:3.24683\n",
      "[192]\teval-mlogloss:3.24701\n",
      "[193]\teval-mlogloss:3.24721\n",
      "[194]\teval-mlogloss:3.24737\n",
      "[195]\teval-mlogloss:3.24753\n",
      "[196]\teval-mlogloss:3.24766\n",
      "[197]\teval-mlogloss:3.24779\n",
      "[198]\teval-mlogloss:3.24796\n",
      "[199]\teval-mlogloss:3.24812\n",
      "[200]\teval-mlogloss:3.24827\n",
      "[201]\teval-mlogloss:3.24842\n",
      "[202]\teval-mlogloss:3.24862\n",
      "[203]\teval-mlogloss:3.24877\n",
      "[204]\teval-mlogloss:3.24890\n",
      "[205]\teval-mlogloss:3.24904\n",
      "[206]\teval-mlogloss:3.24920\n",
      "[207]\teval-mlogloss:3.24932\n",
      "[208]\teval-mlogloss:3.24948\n",
      "[209]\teval-mlogloss:3.24952\n",
      "[210]\teval-mlogloss:3.24974\n",
      "[211]\teval-mlogloss:3.24993\n",
      "[212]\teval-mlogloss:3.25011\n",
      "[213]\teval-mlogloss:3.25035\n",
      "[214]\teval-mlogloss:3.25045\n",
      "[215]\teval-mlogloss:3.25066\n",
      "[216]\teval-mlogloss:3.25083\n",
      "[217]\teval-mlogloss:3.25096\n",
      "[218]\teval-mlogloss:3.25110\n",
      "[219]\teval-mlogloss:3.25122\n",
      "[220]\teval-mlogloss:3.25129\n",
      "[221]\teval-mlogloss:3.25146\n",
      "[222]\teval-mlogloss:3.25156\n",
      "[223]\teval-mlogloss:3.25162\n",
      "[224]\teval-mlogloss:3.25176\n",
      "[225]\teval-mlogloss:3.25189\n",
      "[226]\teval-mlogloss:3.25212\n",
      "[227]\teval-mlogloss:3.25225\n",
      "[228]\teval-mlogloss:3.25241\n",
      "[229]\teval-mlogloss:3.25257\n",
      "[230]\teval-mlogloss:3.25265\n",
      "[231]\teval-mlogloss:3.25286\n",
      "[232]\teval-mlogloss:3.25298\n",
      "[233]\teval-mlogloss:3.25319\n",
      "[234]\teval-mlogloss:3.25336\n",
      "[235]\teval-mlogloss:3.25356\n",
      "[236]\teval-mlogloss:3.25372\n",
      "[237]\teval-mlogloss:3.25390\n",
      "[238]\teval-mlogloss:3.25402\n",
      "[239]\teval-mlogloss:3.25414\n",
      "[240]\teval-mlogloss:3.25426\n",
      "[241]\teval-mlogloss:3.25437\n",
      "[242]\teval-mlogloss:3.25454\n",
      "[243]\teval-mlogloss:3.25466\n",
      "[244]\teval-mlogloss:3.25479\n",
      "[245]\teval-mlogloss:3.25497\n",
      "[246]\teval-mlogloss:3.25502\n",
      "[247]\teval-mlogloss:3.25515\n",
      "[248]\teval-mlogloss:3.25526\n",
      "[249]\teval-mlogloss:3.25534\n",
      "[250]\teval-mlogloss:3.25547\n",
      "[251]\teval-mlogloss:3.25564\n",
      "[252]\teval-mlogloss:3.25576\n",
      "[253]\teval-mlogloss:3.25591\n",
      "[254]\teval-mlogloss:3.25601\n",
      "[255]\teval-mlogloss:3.25615\n",
      "[256]\teval-mlogloss:3.25638\n",
      "[257]\teval-mlogloss:3.25653\n",
      "[258]\teval-mlogloss:3.25667\n",
      "[259]\teval-mlogloss:3.25689\n",
      "[260]\teval-mlogloss:3.25704\n",
      "[261]\teval-mlogloss:3.25717\n",
      "[262]\teval-mlogloss:3.25731\n",
      "[263]\teval-mlogloss:3.25748\n",
      "[264]\teval-mlogloss:3.25764\n",
      "[265]\teval-mlogloss:3.25783\n",
      "[266]\teval-mlogloss:3.25793\n",
      "[267]\teval-mlogloss:3.25818\n",
      "[268]\teval-mlogloss:3.25837\n",
      "[269]\teval-mlogloss:3.25849\n",
      "[270]\teval-mlogloss:3.25868\n",
      "[271]\teval-mlogloss:3.25888\n",
      "[272]\teval-mlogloss:3.25905\n",
      "[273]\teval-mlogloss:3.25919\n",
      "[274]\teval-mlogloss:3.25937\n",
      "[275]\teval-mlogloss:3.25954\n",
      "[276]\teval-mlogloss:3.25966\n",
      "[277]\teval-mlogloss:3.25984\n",
      "[278]\teval-mlogloss:3.26002\n",
      "[279]\teval-mlogloss:3.26019\n",
      "[280]\teval-mlogloss:3.26041\n",
      "[281]\teval-mlogloss:3.26057\n",
      "[282]\teval-mlogloss:3.26075\n",
      "[283]\teval-mlogloss:3.26091\n",
      "[284]\teval-mlogloss:3.26105\n",
      "[285]\teval-mlogloss:3.26118\n",
      "[286]\teval-mlogloss:3.26141\n",
      "[287]\teval-mlogloss:3.26158\n",
      "[288]\teval-mlogloss:3.26176\n",
      "[289]\teval-mlogloss:3.26197\n",
      "[290]\teval-mlogloss:3.26219\n",
      "[291]\teval-mlogloss:3.26237\n",
      "[292]\teval-mlogloss:3.26254\n",
      "[293]\teval-mlogloss:3.26269\n",
      "[294]\teval-mlogloss:3.26281\n",
      "[295]\teval-mlogloss:3.26295\n",
      "[296]\teval-mlogloss:3.26307\n",
      "[297]\teval-mlogloss:3.26321\n",
      "[298]\teval-mlogloss:3.26338\n",
      "[299]\teval-mlogloss:3.26358\n",
      "[300]\teval-mlogloss:3.26378\n",
      "[301]\teval-mlogloss:3.26398\n",
      "[302]\teval-mlogloss:3.26415\n",
      "[303]\teval-mlogloss:3.26434\n",
      "[304]\teval-mlogloss:3.26450\n",
      "[305]\teval-mlogloss:3.26466\n",
      "[306]\teval-mlogloss:3.26480\n",
      "[307]\teval-mlogloss:3.26491\n",
      "[308]\teval-mlogloss:3.26510\n",
      "[309]\teval-mlogloss:3.26526\n",
      "[310]\teval-mlogloss:3.26541\n",
      "[311]\teval-mlogloss:3.26560\n",
      "[312]\teval-mlogloss:3.26575\n",
      "[313]\teval-mlogloss:3.26589\n",
      "[314]\teval-mlogloss:3.26602\n",
      "[315]\teval-mlogloss:3.26620\n",
      "[316]\teval-mlogloss:3.26640\n",
      "[317]\teval-mlogloss:3.26665\n",
      "[318]\teval-mlogloss:3.26685\n",
      "[319]\teval-mlogloss:3.26701\n",
      "[320]\teval-mlogloss:3.26718\n",
      "[321]\teval-mlogloss:3.26731\n",
      "[322]\teval-mlogloss:3.26748\n",
      "[323]\teval-mlogloss:3.26770\n",
      "[324]\teval-mlogloss:3.26789\n",
      "[325]\teval-mlogloss:3.26809\n",
      "[326]\teval-mlogloss:3.26823\n",
      "[327]\teval-mlogloss:3.26842\n",
      "[328]\teval-mlogloss:3.26850\n",
      "[329]\teval-mlogloss:3.26867\n",
      "[330]\teval-mlogloss:3.26888\n",
      "[331]\teval-mlogloss:3.26901\n",
      "[332]\teval-mlogloss:3.26917\n",
      "[333]\teval-mlogloss:3.26925\n",
      "[334]\teval-mlogloss:3.26944\n",
      "[335]\teval-mlogloss:3.26964\n",
      "[336]\teval-mlogloss:3.26985\n",
      "[337]\teval-mlogloss:3.26995\n",
      "[338]\teval-mlogloss:3.27012\n",
      "[339]\teval-mlogloss:3.27028\n",
      "[340]\teval-mlogloss:3.27050\n",
      "[341]\teval-mlogloss:3.27063\n",
      "[342]\teval-mlogloss:3.27074\n",
      "[343]\teval-mlogloss:3.27088\n",
      "[344]\teval-mlogloss:3.27105\n",
      "[345]\teval-mlogloss:3.27125\n",
      "[346]\teval-mlogloss:3.27138\n",
      "[347]\teval-mlogloss:3.27155\n",
      "[348]\teval-mlogloss:3.27170\n",
      "[349]\teval-mlogloss:3.27182\n",
      "[350]\teval-mlogloss:3.27201\n",
      "[351]\teval-mlogloss:3.27223\n",
      "[352]\teval-mlogloss:3.27238\n",
      "[353]\teval-mlogloss:3.27253\n",
      "[354]\teval-mlogloss:3.27270\n",
      "[355]\teval-mlogloss:3.27287\n",
      "[356]\teval-mlogloss:3.27302\n",
      "[357]\teval-mlogloss:3.27311\n",
      "[358]\teval-mlogloss:3.27331\n",
      "[359]\teval-mlogloss:3.27344\n",
      "[360]\teval-mlogloss:3.27354\n",
      "[361]\teval-mlogloss:3.27370\n",
      "[362]\teval-mlogloss:3.27388\n",
      "[363]\teval-mlogloss:3.27400\n",
      "[364]\teval-mlogloss:3.27417\n",
      "[365]\teval-mlogloss:3.27431\n",
      "[366]\teval-mlogloss:3.27439\n",
      "[367]\teval-mlogloss:3.27450\n",
      "[368]\teval-mlogloss:3.27468\n",
      "[369]\teval-mlogloss:3.27485\n",
      "[370]\teval-mlogloss:3.27499\n",
      "[371]\teval-mlogloss:3.27509\n",
      "[372]\teval-mlogloss:3.27519\n",
      "[373]\teval-mlogloss:3.27538\n",
      "[374]\teval-mlogloss:3.27551\n",
      "[375]\teval-mlogloss:3.27572\n",
      "[376]\teval-mlogloss:3.27587\n",
      "[377]\teval-mlogloss:3.27603\n",
      "[378]\teval-mlogloss:3.27618\n",
      "[379]\teval-mlogloss:3.27632\n",
      "[380]\teval-mlogloss:3.27645\n",
      "[381]\teval-mlogloss:3.27657\n",
      "[382]\teval-mlogloss:3.27674\n",
      "[383]\teval-mlogloss:3.27692\n",
      "[384]\teval-mlogloss:3.27707\n",
      "[385]\teval-mlogloss:3.27723\n",
      "[386]\teval-mlogloss:3.27741\n",
      "[387]\teval-mlogloss:3.27752\n",
      "[388]\teval-mlogloss:3.27763\n",
      "[389]\teval-mlogloss:3.27775\n",
      "[390]\teval-mlogloss:3.27790\n",
      "[391]\teval-mlogloss:3.27812\n",
      "[392]\teval-mlogloss:3.27828\n",
      "[393]\teval-mlogloss:3.27850\n",
      "[394]\teval-mlogloss:3.27859\n",
      "[395]\teval-mlogloss:3.27867\n",
      "[396]\teval-mlogloss:3.27887\n",
      "[397]\teval-mlogloss:3.27900\n",
      "[398]\teval-mlogloss:3.27912\n",
      "[399]\teval-mlogloss:3.27927\n",
      "[400]\teval-mlogloss:3.27941\n",
      "[401]\teval-mlogloss:3.27964\n",
      "[402]\teval-mlogloss:3.27976\n",
      "[403]\teval-mlogloss:3.27990\n",
      "[404]\teval-mlogloss:3.28003\n",
      "[405]\teval-mlogloss:3.28019\n",
      "[406]\teval-mlogloss:3.28030\n",
      "[407]\teval-mlogloss:3.28045\n",
      "[408]\teval-mlogloss:3.28059\n",
      "[409]\teval-mlogloss:3.28073\n",
      "[410]\teval-mlogloss:3.28086\n",
      "[411]\teval-mlogloss:3.28100\n",
      "[412]\teval-mlogloss:3.28118\n",
      "[413]\teval-mlogloss:3.28127\n",
      "[414]\teval-mlogloss:3.28149\n",
      "[415]\teval-mlogloss:3.28159\n",
      "[416]\teval-mlogloss:3.28172\n",
      "[417]\teval-mlogloss:3.28194\n",
      "[418]\teval-mlogloss:3.28208\n",
      "[419]\teval-mlogloss:3.28225\n",
      "[420]\teval-mlogloss:3.28244\n",
      "[421]\teval-mlogloss:3.28258\n",
      "[422]\teval-mlogloss:3.28277\n",
      "[423]\teval-mlogloss:3.28297\n",
      "[424]\teval-mlogloss:3.28317\n",
      "[425]\teval-mlogloss:3.28322\n",
      "[426]\teval-mlogloss:3.28342\n",
      "[427]\teval-mlogloss:3.28356\n",
      "[428]\teval-mlogloss:3.28371\n",
      "[429]\teval-mlogloss:3.28386\n",
      "[430]\teval-mlogloss:3.28404\n",
      "[431]\teval-mlogloss:3.28418\n",
      "[432]\teval-mlogloss:3.28439\n",
      "[433]\teval-mlogloss:3.28453\n",
      "[434]\teval-mlogloss:3.28466\n",
      "[435]\teval-mlogloss:3.28481\n",
      "[436]\teval-mlogloss:3.28501\n",
      "[437]\teval-mlogloss:3.28523\n",
      "[438]\teval-mlogloss:3.28535\n",
      "[439]\teval-mlogloss:3.28555\n",
      "[440]\teval-mlogloss:3.28570\n",
      "[441]\teval-mlogloss:3.28582\n",
      "[442]\teval-mlogloss:3.28600\n",
      "[443]\teval-mlogloss:3.28620\n",
      "[444]\teval-mlogloss:3.28637\n",
      "[445]\teval-mlogloss:3.28649\n",
      "[446]\teval-mlogloss:3.28668\n",
      "[447]\teval-mlogloss:3.28681\n",
      "[448]\teval-mlogloss:3.28694\n",
      "[449]\teval-mlogloss:3.28709\n",
      "[450]\teval-mlogloss:3.28724\n",
      "[451]\teval-mlogloss:3.28735\n",
      "[452]\teval-mlogloss:3.28744\n",
      "[453]\teval-mlogloss:3.28759\n",
      "[454]\teval-mlogloss:3.28776\n",
      "[455]\teval-mlogloss:3.28790\n",
      "[456]\teval-mlogloss:3.28802\n",
      "[457]\teval-mlogloss:3.28819\n",
      "[458]\teval-mlogloss:3.28842\n",
      "[459]\teval-mlogloss:3.28862\n",
      "[460]\teval-mlogloss:3.28875\n",
      "[461]\teval-mlogloss:3.28889\n",
      "[462]\teval-mlogloss:3.28902\n",
      "[463]\teval-mlogloss:3.28917\n",
      "[464]\teval-mlogloss:3.28930\n",
      "[465]\teval-mlogloss:3.28944\n",
      "[466]\teval-mlogloss:3.28963\n",
      "[467]\teval-mlogloss:3.28977\n",
      "[468]\teval-mlogloss:3.28990\n",
      "[469]\teval-mlogloss:3.29011\n",
      "[470]\teval-mlogloss:3.29023\n",
      "[471]\teval-mlogloss:3.29035\n",
      "[472]\teval-mlogloss:3.29048\n",
      "[473]\teval-mlogloss:3.29061\n",
      "[474]\teval-mlogloss:3.29078\n",
      "[475]\teval-mlogloss:3.29096\n",
      "[476]\teval-mlogloss:3.29117\n",
      "[477]\teval-mlogloss:3.29135\n",
      "[478]\teval-mlogloss:3.29147\n",
      "[479]\teval-mlogloss:3.29164\n",
      "[480]\teval-mlogloss:3.29179\n",
      "[481]\teval-mlogloss:3.29200\n",
      "[482]\teval-mlogloss:3.29212\n",
      "[483]\teval-mlogloss:3.29227\n",
      "[484]\teval-mlogloss:3.29241\n",
      "[485]\teval-mlogloss:3.29256\n",
      "[486]\teval-mlogloss:3.29274\n",
      "[487]\teval-mlogloss:3.29291\n",
      "[488]\teval-mlogloss:3.29295\n",
      "[489]\teval-mlogloss:3.29306\n",
      "[490]\teval-mlogloss:3.29319\n",
      "[491]\teval-mlogloss:3.29336\n",
      "[492]\teval-mlogloss:3.29349\n",
      "[493]\teval-mlogloss:3.29359\n",
      "[494]\teval-mlogloss:3.29372\n",
      "[495]\teval-mlogloss:3.29386\n",
      "[496]\teval-mlogloss:3.29396\n",
      "[497]\teval-mlogloss:3.29408\n",
      "[498]\teval-mlogloss:3.29421\n",
      "[499]\teval-mlogloss:3.29440\n",
      "[500]\teval-mlogloss:3.29452\n",
      "[501]\teval-mlogloss:3.29472\n",
      "[502]\teval-mlogloss:3.29479\n",
      "[503]\teval-mlogloss:3.29484\n",
      "[504]\teval-mlogloss:3.29497\n",
      "[505]\teval-mlogloss:3.29517\n",
      "[506]\teval-mlogloss:3.29521\n",
      "[507]\teval-mlogloss:3.29538\n",
      "[508]\teval-mlogloss:3.29546\n",
      "[509]\teval-mlogloss:3.29559\n",
      "[510]\teval-mlogloss:3.29574\n",
      "[511]\teval-mlogloss:3.29591\n",
      "[512]\teval-mlogloss:3.29599\n",
      "[513]\teval-mlogloss:3.29614\n",
      "[514]\teval-mlogloss:3.29625\n",
      "[515]\teval-mlogloss:3.29645\n",
      "[516]\teval-mlogloss:3.29662\n",
      "[517]\teval-mlogloss:3.29669\n",
      "[518]\teval-mlogloss:3.29678\n",
      "[519]\teval-mlogloss:3.29690\n",
      "[520]\teval-mlogloss:3.29711\n",
      "[521]\teval-mlogloss:3.29721\n",
      "[522]\teval-mlogloss:3.29734\n",
      "[523]\teval-mlogloss:3.29749\n",
      "[524]\teval-mlogloss:3.29764\n",
      "[525]\teval-mlogloss:3.29777\n",
      "[526]\teval-mlogloss:3.29790\n",
      "[527]\teval-mlogloss:3.29806\n",
      "[528]\teval-mlogloss:3.29833\n",
      "[529]\teval-mlogloss:3.29843\n",
      "[530]\teval-mlogloss:3.29854\n",
      "[531]\teval-mlogloss:3.29870\n",
      "[532]\teval-mlogloss:3.29885\n",
      "[533]\teval-mlogloss:3.29891\n",
      "[534]\teval-mlogloss:3.29908\n",
      "[535]\teval-mlogloss:3.29924\n",
      "[536]\teval-mlogloss:3.29941\n",
      "[537]\teval-mlogloss:3.29948\n",
      "[538]\teval-mlogloss:3.29963\n",
      "[539]\teval-mlogloss:3.29974\n",
      "[540]\teval-mlogloss:3.29985\n",
      "[541]\teval-mlogloss:3.30003\n",
      "[542]\teval-mlogloss:3.30013\n",
      "[543]\teval-mlogloss:3.30027\n",
      "[544]\teval-mlogloss:3.30043\n",
      "[545]\teval-mlogloss:3.30061\n",
      "[546]\teval-mlogloss:3.30072\n",
      "[547]\teval-mlogloss:3.30081\n",
      "[548]\teval-mlogloss:3.30093\n",
      "[549]\teval-mlogloss:3.30105\n",
      "[550]\teval-mlogloss:3.30125\n",
      "[551]\teval-mlogloss:3.30137\n",
      "[552]\teval-mlogloss:3.30146\n",
      "[553]\teval-mlogloss:3.30163\n",
      "[554]\teval-mlogloss:3.30182\n",
      "[555]\teval-mlogloss:3.30193\n",
      "[556]\teval-mlogloss:3.30215\n",
      "[557]\teval-mlogloss:3.30236\n",
      "[558]\teval-mlogloss:3.30256\n",
      "[559]\teval-mlogloss:3.30270\n",
      "[560]\teval-mlogloss:3.30288\n",
      "[561]\teval-mlogloss:3.30302\n",
      "[562]\teval-mlogloss:3.30319\n",
      "[563]\teval-mlogloss:3.30336\n",
      "[564]\teval-mlogloss:3.30354\n",
      "[565]\teval-mlogloss:3.30374\n",
      "[566]\teval-mlogloss:3.30394\n",
      "[567]\teval-mlogloss:3.30410\n",
      "[568]\teval-mlogloss:3.30424\n",
      "[569]\teval-mlogloss:3.30439\n",
      "[570]\teval-mlogloss:3.30454\n",
      "[571]\teval-mlogloss:3.30468\n",
      "[572]\teval-mlogloss:3.30484\n",
      "[573]\teval-mlogloss:3.30497\n",
      "[574]\teval-mlogloss:3.30509\n",
      "[575]\teval-mlogloss:3.30525\n",
      "[576]\teval-mlogloss:3.30544\n",
      "[577]\teval-mlogloss:3.30555\n",
      "[578]\teval-mlogloss:3.30566\n",
      "[579]\teval-mlogloss:3.30582\n",
      "[580]\teval-mlogloss:3.30601\n",
      "[581]\teval-mlogloss:3.30621\n",
      "[582]\teval-mlogloss:3.30640\n",
      "[583]\teval-mlogloss:3.30660\n",
      "[584]\teval-mlogloss:3.30674\n",
      "[585]\teval-mlogloss:3.30692\n",
      "[586]\teval-mlogloss:3.30706\n",
      "[587]\teval-mlogloss:3.30720\n",
      "[588]\teval-mlogloss:3.30733\n",
      "[589]\teval-mlogloss:3.30749\n",
      "[590]\teval-mlogloss:3.30764\n",
      "[591]\teval-mlogloss:3.30775\n",
      "[592]\teval-mlogloss:3.30789\n",
      "[593]\teval-mlogloss:3.30805\n",
      "[594]\teval-mlogloss:3.30820\n",
      "[595]\teval-mlogloss:3.30838\n",
      "[596]\teval-mlogloss:3.30854\n",
      "[597]\teval-mlogloss:3.30875\n",
      "[598]\teval-mlogloss:3.30891\n",
      "[599]\teval-mlogloss:3.30911\n",
      "[600]\teval-mlogloss:3.30924\n",
      "[601]\teval-mlogloss:3.30935\n",
      "[602]\teval-mlogloss:3.30949\n",
      "[603]\teval-mlogloss:3.30969\n",
      "[604]\teval-mlogloss:3.30982\n",
      "[605]\teval-mlogloss:3.30998\n",
      "[606]\teval-mlogloss:3.31018\n",
      "[607]\teval-mlogloss:3.31038\n",
      "[608]\teval-mlogloss:3.31052\n",
      "[609]\teval-mlogloss:3.31061\n",
      "[610]\teval-mlogloss:3.31070\n",
      "[611]\teval-mlogloss:3.31079\n",
      "[612]\teval-mlogloss:3.31094\n",
      "[613]\teval-mlogloss:3.31110\n",
      "[614]\teval-mlogloss:3.31119\n",
      "[615]\teval-mlogloss:3.31136\n",
      "[616]\teval-mlogloss:3.31148\n",
      "[617]\teval-mlogloss:3.31161\n",
      "[618]\teval-mlogloss:3.31180\n",
      "[619]\teval-mlogloss:3.31195\n",
      "[620]\teval-mlogloss:3.31210\n",
      "[621]\teval-mlogloss:3.31226\n",
      "[622]\teval-mlogloss:3.31241\n",
      "[623]\teval-mlogloss:3.31260\n",
      "[624]\teval-mlogloss:3.31276\n",
      "[625]\teval-mlogloss:3.31297\n",
      "[626]\teval-mlogloss:3.31312\n",
      "[627]\teval-mlogloss:3.31325\n",
      "[628]\teval-mlogloss:3.31337\n",
      "[629]\teval-mlogloss:3.31357\n",
      "[630]\teval-mlogloss:3.31373\n",
      "[631]\teval-mlogloss:3.31386\n",
      "[632]\teval-mlogloss:3.31404\n",
      "[633]\teval-mlogloss:3.31421\n",
      "[634]\teval-mlogloss:3.31438\n",
      "[635]\teval-mlogloss:3.31457\n",
      "[636]\teval-mlogloss:3.31465\n",
      "[637]\teval-mlogloss:3.31482\n",
      "[638]\teval-mlogloss:3.31493\n",
      "[639]\teval-mlogloss:3.31510\n",
      "[640]\teval-mlogloss:3.31525\n",
      "[641]\teval-mlogloss:3.31534\n",
      "[642]\teval-mlogloss:3.31552\n",
      "[643]\teval-mlogloss:3.31560\n",
      "[644]\teval-mlogloss:3.31580\n",
      "[645]\teval-mlogloss:3.31594\n",
      "[646]\teval-mlogloss:3.31609\n",
      "[647]\teval-mlogloss:3.31622\n",
      "[648]\teval-mlogloss:3.31633\n",
      "[649]\teval-mlogloss:3.31646\n",
      "[650]\teval-mlogloss:3.31656\n",
      "[651]\teval-mlogloss:3.31672\n",
      "[652]\teval-mlogloss:3.31690\n",
      "[653]\teval-mlogloss:3.31697\n",
      "[654]\teval-mlogloss:3.31713\n",
      "[655]\teval-mlogloss:3.31731\n",
      "[656]\teval-mlogloss:3.31741\n",
      "[657]\teval-mlogloss:3.31756\n",
      "[658]\teval-mlogloss:3.31770\n",
      "[659]\teval-mlogloss:3.31794\n",
      "[660]\teval-mlogloss:3.31813\n",
      "[661]\teval-mlogloss:3.31830\n",
      "[662]\teval-mlogloss:3.31844\n",
      "[663]\teval-mlogloss:3.31857\n",
      "[664]\teval-mlogloss:3.31871\n",
      "[665]\teval-mlogloss:3.31895\n",
      "[666]\teval-mlogloss:3.31910\n",
      "[667]\teval-mlogloss:3.31931\n",
      "[668]\teval-mlogloss:3.31947\n",
      "[669]\teval-mlogloss:3.31960\n",
      "[670]\teval-mlogloss:3.31977\n",
      "[671]\teval-mlogloss:3.31982\n",
      "[672]\teval-mlogloss:3.32000\n",
      "[673]\teval-mlogloss:3.32011\n",
      "[674]\teval-mlogloss:3.32024\n",
      "[675]\teval-mlogloss:3.32040\n",
      "[676]\teval-mlogloss:3.32057\n",
      "[677]\teval-mlogloss:3.32075\n",
      "[678]\teval-mlogloss:3.32087\n",
      "[679]\teval-mlogloss:3.32099\n",
      "[680]\teval-mlogloss:3.32110\n",
      "[681]\teval-mlogloss:3.32130\n",
      "[682]\teval-mlogloss:3.32143\n",
      "[683]\teval-mlogloss:3.32160\n",
      "[684]\teval-mlogloss:3.32176\n",
      "[685]\teval-mlogloss:3.32189\n",
      "[686]\teval-mlogloss:3.32200\n",
      "[687]\teval-mlogloss:3.32217\n",
      "[688]\teval-mlogloss:3.32229\n",
      "[689]\teval-mlogloss:3.32240\n",
      "[690]\teval-mlogloss:3.32254\n",
      "[691]\teval-mlogloss:3.32271\n",
      "[692]\teval-mlogloss:3.32286\n",
      "[693]\teval-mlogloss:3.32298\n",
      "[694]\teval-mlogloss:3.32312\n",
      "[695]\teval-mlogloss:3.32332\n",
      "[696]\teval-mlogloss:3.32344\n",
      "[697]\teval-mlogloss:3.32362\n",
      "[698]\teval-mlogloss:3.32375\n",
      "[699]\teval-mlogloss:3.32390\n",
      "[700]\teval-mlogloss:3.32406\n",
      "[701]\teval-mlogloss:3.32425\n",
      "[702]\teval-mlogloss:3.32436\n",
      "[703]\teval-mlogloss:3.32450\n",
      "[704]\teval-mlogloss:3.32469\n",
      "[705]\teval-mlogloss:3.32482\n",
      "[706]\teval-mlogloss:3.32496\n",
      "[707]\teval-mlogloss:3.32511\n",
      "[708]\teval-mlogloss:3.32529\n",
      "[709]\teval-mlogloss:3.32551\n",
      "[710]\teval-mlogloss:3.32569\n",
      "[711]\teval-mlogloss:3.32589\n",
      "[712]\teval-mlogloss:3.32599\n",
      "[713]\teval-mlogloss:3.32607\n",
      "[714]\teval-mlogloss:3.32627\n",
      "[715]\teval-mlogloss:3.32642\n",
      "[716]\teval-mlogloss:3.32661\n",
      "[717]\teval-mlogloss:3.32672\n",
      "[718]\teval-mlogloss:3.32691\n",
      "[719]\teval-mlogloss:3.32710\n",
      "[720]\teval-mlogloss:3.32724\n",
      "[721]\teval-mlogloss:3.32738\n",
      "[722]\teval-mlogloss:3.32748\n",
      "[723]\teval-mlogloss:3.32761\n",
      "[724]\teval-mlogloss:3.32770\n",
      "[725]\teval-mlogloss:3.32785\n",
      "[726]\teval-mlogloss:3.32801\n",
      "[727]\teval-mlogloss:3.32812\n",
      "[728]\teval-mlogloss:3.32827\n",
      "[729]\teval-mlogloss:3.32840\n",
      "[730]\teval-mlogloss:3.32850\n",
      "[731]\teval-mlogloss:3.32874\n",
      "[732]\teval-mlogloss:3.32885\n",
      "[733]\teval-mlogloss:3.32902\n",
      "[734]\teval-mlogloss:3.32917\n",
      "[735]\teval-mlogloss:3.32936\n",
      "[736]\teval-mlogloss:3.32955\n",
      "[737]\teval-mlogloss:3.32966\n",
      "[738]\teval-mlogloss:3.32981\n",
      "[739]\teval-mlogloss:3.32997\n",
      "[740]\teval-mlogloss:3.33011\n",
      "[741]\teval-mlogloss:3.33023\n",
      "[742]\teval-mlogloss:3.33035\n",
      "[743]\teval-mlogloss:3.33048\n",
      "[744]\teval-mlogloss:3.33067\n",
      "[745]\teval-mlogloss:3.33074\n",
      "[746]\teval-mlogloss:3.33091\n",
      "[747]\teval-mlogloss:3.33110\n",
      "[748]\teval-mlogloss:3.33127\n",
      "[749]\teval-mlogloss:3.33145\n",
      "[750]\teval-mlogloss:3.33156\n",
      "[751]\teval-mlogloss:3.33172\n",
      "[752]\teval-mlogloss:3.33184\n",
      "[753]\teval-mlogloss:3.33203\n",
      "[754]\teval-mlogloss:3.33227\n",
      "[755]\teval-mlogloss:3.33243\n",
      "[756]\teval-mlogloss:3.33263\n",
      "[757]\teval-mlogloss:3.33280\n",
      "[758]\teval-mlogloss:3.33290\n",
      "[759]\teval-mlogloss:3.33307\n",
      "[760]\teval-mlogloss:3.33322\n",
      "[761]\teval-mlogloss:3.33337\n",
      "[762]\teval-mlogloss:3.33357\n",
      "[763]\teval-mlogloss:3.33374\n",
      "[764]\teval-mlogloss:3.33383\n",
      "[765]\teval-mlogloss:3.33400\n",
      "[766]\teval-mlogloss:3.33416\n",
      "[767]\teval-mlogloss:3.33430\n",
      "[768]\teval-mlogloss:3.33448\n",
      "[769]\teval-mlogloss:3.33466\n",
      "[770]\teval-mlogloss:3.33477\n",
      "[771]\teval-mlogloss:3.33492\n",
      "[772]\teval-mlogloss:3.33507\n",
      "[773]\teval-mlogloss:3.33522\n",
      "[774]\teval-mlogloss:3.33536\n",
      "[775]\teval-mlogloss:3.33556\n",
      "[776]\teval-mlogloss:3.33568\n",
      "[777]\teval-mlogloss:3.33582\n",
      "[778]\teval-mlogloss:3.33596\n",
      "[779]\teval-mlogloss:3.33609\n",
      "[780]\teval-mlogloss:3.33623\n",
      "[781]\teval-mlogloss:3.33635\n",
      "[782]\teval-mlogloss:3.33649\n",
      "[783]\teval-mlogloss:3.33665\n",
      "[784]\teval-mlogloss:3.33678\n",
      "[785]\teval-mlogloss:3.33694\n",
      "[786]\teval-mlogloss:3.33705\n",
      "[787]\teval-mlogloss:3.33717\n",
      "[788]\teval-mlogloss:3.33737\n",
      "[789]\teval-mlogloss:3.33747\n",
      "[790]\teval-mlogloss:3.33763\n",
      "[791]\teval-mlogloss:3.33775\n",
      "[792]\teval-mlogloss:3.33787\n",
      "[793]\teval-mlogloss:3.33804\n",
      "[794]\teval-mlogloss:3.33822\n",
      "[795]\teval-mlogloss:3.33835\n",
      "[796]\teval-mlogloss:3.33845\n",
      "[797]\teval-mlogloss:3.33861\n",
      "[798]\teval-mlogloss:3.33879\n",
      "[799]\teval-mlogloss:3.33892\n",
      "[800]\teval-mlogloss:3.33903\n",
      "[801]\teval-mlogloss:3.33920\n",
      "[802]\teval-mlogloss:3.33935\n",
      "[803]\teval-mlogloss:3.33951\n",
      "[804]\teval-mlogloss:3.33960\n",
      "[805]\teval-mlogloss:3.33974\n",
      "[806]\teval-mlogloss:3.33988\n",
      "[807]\teval-mlogloss:3.34010\n",
      "[808]\teval-mlogloss:3.34022\n",
      "[809]\teval-mlogloss:3.34030\n",
      "[810]\teval-mlogloss:3.34051\n",
      "[811]\teval-mlogloss:3.34062\n",
      "[812]\teval-mlogloss:3.34075\n",
      "[813]\teval-mlogloss:3.34083\n",
      "[814]\teval-mlogloss:3.34090\n",
      "[815]\teval-mlogloss:3.34106\n",
      "[816]\teval-mlogloss:3.34120\n",
      "[817]\teval-mlogloss:3.34130\n",
      "[818]\teval-mlogloss:3.34145\n",
      "[819]\teval-mlogloss:3.34169\n",
      "[820]\teval-mlogloss:3.34189\n",
      "[821]\teval-mlogloss:3.34210\n",
      "[822]\teval-mlogloss:3.34218\n",
      "[823]\teval-mlogloss:3.34226\n",
      "[824]\teval-mlogloss:3.34241\n",
      "[825]\teval-mlogloss:3.34261\n",
      "[826]\teval-mlogloss:3.34275\n",
      "[827]\teval-mlogloss:3.34291\n",
      "[828]\teval-mlogloss:3.34311\n",
      "[829]\teval-mlogloss:3.34325\n",
      "[830]\teval-mlogloss:3.34340\n",
      "[831]\teval-mlogloss:3.34349\n",
      "[832]\teval-mlogloss:3.34365\n",
      "[833]\teval-mlogloss:3.34378\n",
      "[834]\teval-mlogloss:3.34391\n",
      "[835]\teval-mlogloss:3.34403\n",
      "[836]\teval-mlogloss:3.34414\n",
      "[837]\teval-mlogloss:3.34426\n",
      "[838]\teval-mlogloss:3.34448\n",
      "[839]\teval-mlogloss:3.34461\n",
      "[840]\teval-mlogloss:3.34478\n",
      "[841]\teval-mlogloss:3.34488\n",
      "[842]\teval-mlogloss:3.34505\n",
      "[843]\teval-mlogloss:3.34518\n",
      "[844]\teval-mlogloss:3.34539\n",
      "[845]\teval-mlogloss:3.34550\n",
      "[846]\teval-mlogloss:3.34561\n",
      "[847]\teval-mlogloss:3.34575\n",
      "[848]\teval-mlogloss:3.34588\n",
      "[849]\teval-mlogloss:3.34608\n",
      "[850]\teval-mlogloss:3.34622\n",
      "[851]\teval-mlogloss:3.34639\n",
      "[852]\teval-mlogloss:3.34650\n",
      "[853]\teval-mlogloss:3.34668\n",
      "[854]\teval-mlogloss:3.34681\n",
      "[855]\teval-mlogloss:3.34693\n",
      "[856]\teval-mlogloss:3.34708\n",
      "[857]\teval-mlogloss:3.34720\n",
      "[858]\teval-mlogloss:3.34734\n",
      "[859]\teval-mlogloss:3.34753\n",
      "[860]\teval-mlogloss:3.34768\n",
      "[861]\teval-mlogloss:3.34784\n",
      "[862]\teval-mlogloss:3.34798\n",
      "[863]\teval-mlogloss:3.34811\n",
      "[864]\teval-mlogloss:3.34825\n",
      "[865]\teval-mlogloss:3.34842\n",
      "[866]\teval-mlogloss:3.34852\n",
      "[867]\teval-mlogloss:3.34868\n",
      "[868]\teval-mlogloss:3.34876\n",
      "[869]\teval-mlogloss:3.34896\n",
      "[870]\teval-mlogloss:3.34903\n",
      "[871]\teval-mlogloss:3.34922\n",
      "[872]\teval-mlogloss:3.34934\n",
      "[873]\teval-mlogloss:3.34950\n",
      "[874]\teval-mlogloss:3.34963\n",
      "[875]\teval-mlogloss:3.34976\n",
      "[876]\teval-mlogloss:3.34987\n",
      "[877]\teval-mlogloss:3.35000\n",
      "[878]\teval-mlogloss:3.35013\n",
      "[879]\teval-mlogloss:3.35030\n",
      "[880]\teval-mlogloss:3.35048\n",
      "[881]\teval-mlogloss:3.35063\n",
      "[882]\teval-mlogloss:3.35080\n",
      "[883]\teval-mlogloss:3.35098\n",
      "[884]\teval-mlogloss:3.35116\n",
      "[885]\teval-mlogloss:3.35130\n",
      "[886]\teval-mlogloss:3.35145\n",
      "[887]\teval-mlogloss:3.35157\n",
      "[888]\teval-mlogloss:3.35170\n",
      "[889]\teval-mlogloss:3.35178\n",
      "[890]\teval-mlogloss:3.35194\n",
      "[891]\teval-mlogloss:3.35211\n",
      "[892]\teval-mlogloss:3.35225\n",
      "[893]\teval-mlogloss:3.35242\n",
      "[894]\teval-mlogloss:3.35259\n",
      "[895]\teval-mlogloss:3.35272\n",
      "[896]\teval-mlogloss:3.35286\n",
      "[897]\teval-mlogloss:3.35295\n",
      "[898]\teval-mlogloss:3.35312\n",
      "[899]\teval-mlogloss:3.35325\n",
      "[900]\teval-mlogloss:3.35342\n",
      "[901]\teval-mlogloss:3.35363\n",
      "[902]\teval-mlogloss:3.35378\n",
      "[903]\teval-mlogloss:3.35389\n",
      "[904]\teval-mlogloss:3.35401\n",
      "[905]\teval-mlogloss:3.35412\n",
      "[906]\teval-mlogloss:3.35425\n",
      "[907]\teval-mlogloss:3.35441\n",
      "[908]\teval-mlogloss:3.35453\n",
      "[909]\teval-mlogloss:3.35468\n",
      "[910]\teval-mlogloss:3.35487\n",
      "[911]\teval-mlogloss:3.35501\n",
      "[912]\teval-mlogloss:3.35525\n",
      "[913]\teval-mlogloss:3.35539\n",
      "[914]\teval-mlogloss:3.35546\n",
      "[915]\teval-mlogloss:3.35565\n",
      "[916]\teval-mlogloss:3.35579\n",
      "[917]\teval-mlogloss:3.35596\n",
      "[918]\teval-mlogloss:3.35613\n",
      "[919]\teval-mlogloss:3.35630\n",
      "[920]\teval-mlogloss:3.35647\n",
      "[921]\teval-mlogloss:3.35660\n",
      "[922]\teval-mlogloss:3.35676\n",
      "[923]\teval-mlogloss:3.35691\n",
      "[924]\teval-mlogloss:3.35706\n",
      "[925]\teval-mlogloss:3.35720\n",
      "[926]\teval-mlogloss:3.35733\n",
      "[927]\teval-mlogloss:3.35742\n",
      "[928]\teval-mlogloss:3.35757\n",
      "[929]\teval-mlogloss:3.35774\n",
      "[930]\teval-mlogloss:3.35786\n",
      "[931]\teval-mlogloss:3.35807\n",
      "[932]\teval-mlogloss:3.35822\n",
      "[933]\teval-mlogloss:3.35842\n",
      "[934]\teval-mlogloss:3.35857\n",
      "[935]\teval-mlogloss:3.35874\n",
      "[936]\teval-mlogloss:3.35881\n",
      "[937]\teval-mlogloss:3.35899\n",
      "[938]\teval-mlogloss:3.35917\n",
      "[939]\teval-mlogloss:3.35927\n",
      "[940]\teval-mlogloss:3.35943\n",
      "[941]\teval-mlogloss:3.35957\n",
      "[942]\teval-mlogloss:3.35972\n",
      "[943]\teval-mlogloss:3.35988\n",
      "[944]\teval-mlogloss:3.36007\n",
      "[945]\teval-mlogloss:3.36022\n",
      "[946]\teval-mlogloss:3.36041\n",
      "[947]\teval-mlogloss:3.36048\n",
      "[948]\teval-mlogloss:3.36067\n",
      "[949]\teval-mlogloss:3.36086\n",
      "[950]\teval-mlogloss:3.36099\n",
      "[951]\teval-mlogloss:3.36116\n",
      "[952]\teval-mlogloss:3.36130\n",
      "[953]\teval-mlogloss:3.36147\n",
      "[954]\teval-mlogloss:3.36157\n",
      "[955]\teval-mlogloss:3.36173\n",
      "[956]\teval-mlogloss:3.36195\n",
      "[957]\teval-mlogloss:3.36206\n",
      "[958]\teval-mlogloss:3.36216\n",
      "[959]\teval-mlogloss:3.36228\n",
      "[960]\teval-mlogloss:3.36242\n",
      "[961]\teval-mlogloss:3.36259\n",
      "[962]\teval-mlogloss:3.36272\n",
      "[963]\teval-mlogloss:3.36286\n",
      "[964]\teval-mlogloss:3.36302\n",
      "[965]\teval-mlogloss:3.36318\n",
      "[966]\teval-mlogloss:3.36337\n",
      "[967]\teval-mlogloss:3.36349\n",
      "[968]\teval-mlogloss:3.36359\n",
      "[969]\teval-mlogloss:3.36378\n",
      "[970]\teval-mlogloss:3.36397\n",
      "[971]\teval-mlogloss:3.36410\n",
      "[972]\teval-mlogloss:3.36424\n",
      "[973]\teval-mlogloss:3.36443\n",
      "[974]\teval-mlogloss:3.36457\n",
      "[975]\teval-mlogloss:3.36468\n",
      "[976]\teval-mlogloss:3.36481\n",
      "[977]\teval-mlogloss:3.36493\n",
      "[978]\teval-mlogloss:3.36509\n",
      "[979]\teval-mlogloss:3.36529\n",
      "[980]\teval-mlogloss:3.36546\n",
      "[981]\teval-mlogloss:3.36554\n",
      "[982]\teval-mlogloss:3.36567\n",
      "[983]\teval-mlogloss:3.36585\n",
      "[984]\teval-mlogloss:3.36600\n",
      "[985]\teval-mlogloss:3.36613\n",
      "[986]\teval-mlogloss:3.36626\n",
      "[987]\teval-mlogloss:3.36639\n",
      "[988]\teval-mlogloss:3.36650\n",
      "[989]\teval-mlogloss:3.36667\n",
      "[990]\teval-mlogloss:3.36676\n",
      "[991]\teval-mlogloss:3.36687\n",
      "[992]\teval-mlogloss:3.36701\n",
      "[993]\teval-mlogloss:3.36710\n",
      "[994]\teval-mlogloss:3.36724\n",
      "[995]\teval-mlogloss:3.36743\n",
      "[996]\teval-mlogloss:3.36754\n",
      "[997]\teval-mlogloss:3.36766\n",
      "[998]\teval-mlogloss:3.36777\n",
      "[999]\teval-mlogloss:3.36795\n",
      "Blended Accuracy: 0.06580330689094906\n",
      "Blended F1 Score: 0.03820608729978196\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert data into DMatrix with device specified\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, nthread=-1)\n",
    "dval = xgb.DMatrix(X_val, label=y_val, nthread=-1)\n",
    "\n",
    "# Specify the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'multi:softprob',  # multi-class classification\n",
    "    'num_class': len(label_encoder.classes_),\n",
    "    'tree_method': 'hist',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'verbosity': 1,\n",
    "    'predictor': 'gpu_predictor',  # Use GPU for predictions\n",
    "    'gpu_id': 0  # Assuming you want to use the first GPU\n",
    "}\n",
    "\n",
    "# Train the model with the DMatrix format\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dval, 'eval')], early_stopping_rounds=20, verbose_eval=True)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = xgb_model.predict(dval)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "accuracy = accuracy_score(y_val, y_val_pred_labels)\n",
    "f1 = f1_score(y_val, y_val_pred_labels, average='weighted')\n",
    "\n",
    "print(f\"Blended Accuracy: {accuracy}\")\n",
    "print(f\"Blended F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Convert data into DMatrix with device specified\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(\u001b[43mX_train\u001b[49m, label\u001b[38;5;241m=\u001b[39my_train, nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m dval \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_val, label\u001b[38;5;241m=\u001b[39my_val, nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Specify the parameters for the XGBoost model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert data into DMatrix with device specified\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, nthread=-1)\n",
    "dval = xgb.DMatrix(X_val, label=y_val, nthread=-1)\n",
    "\n",
    "# Specify the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'multi:softprob',  # multi-class classification\n",
    "    'num_class': len(label_encoder.classes_),\n",
    "    'tree_method': 'hist',\n",
    "    'max_depth': 6,  # Reduced max depth to prevent overfitting\n",
    "    'learning_rate': 0.05,  # Lower learning rate\n",
    "    'subsample': 0.7,  # Lower subsample to reduce overfitting\n",
    "    'colsample_bytree': 0.7,  # Lower colsample_bytree\n",
    "    'reg_alpha': 1,  # Add L1 regularization term\n",
    "    'reg_lambda': 10,  # Add L2 regularization term\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'verbosity': 1,\n",
    "    'predictor': 'gpu_predictor',  # Use GPU for predictions\n",
    "    'gpu_id': 0  # Assuming you want to use the first GPU\n",
    "}\n",
    "\n",
    "# Train the model with the DMatrix format, using early stopping\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dval, 'eval')],\n",
    "                      early_stopping_rounds=50,  # Stop if no improvement in 50 rounds\n",
    "                      verbose_eval=10)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = xgb_model.predict(dval)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "accuracy = accuracy_score(y_val, y_val_pred_labels)\n",
    "f1 = f1_score(y_val, y_val_pred_labels, average='weighted')\n",
    "\n",
    "print(f\"Blended Accuracy: {accuracy}\")\n",
    "print(f\"Blended F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blended Accuracy: 0.043996304437860394\n",
      "Blended F1 Score: 0.04406909023573695\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set using MiniLM's output probabilities\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "minilm_val_pred = np.argmax(X_val, axis=1)\n",
    "\n",
    "# Predict on the validation set using XGBoost\n",
    "xgb_val_pred_prob = xgb_model.predict(xgb.DMatrix(X_val))\n",
    "xgb_val_pred = np.argmax(xgb_val_pred_prob, axis=1)\n",
    "\n",
    "# Perform blending (simple average)\n",
    "final_blended_pred = np.argmax(\n",
    "    (0.5 * X_val) + (0.5 * xgb_val_pred_prob), axis=1)\n",
    "\n",
    "# Evaluate blended predictions\n",
    "blended_accuracy = accuracy_score(y_val, final_blended_pred)\n",
    "blended_f1 = f1_score(y_val, final_blended_pred, average='weighted')\n",
    "\n",
    "print(f\"Blended Accuracy: {blended_accuracy}\")\n",
    "print(f\"Blended F1 Score: {blended_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending with XGBoost completed. Final predictions saved to 'blended_submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset predictions from MiniLM\n",
    "minilm_test_probabilities = np.load('minilm_probabilities.npy')\n",
    "\n",
    "# Ensure minilm_test_probabilities is in the correct shape for XGBoost prediction\n",
    "dtest = xgb.DMatrix(minilm_test_probabilities)\n",
    "\n",
    "# Get XGBoost predictions\n",
    "xgb_test_pred_prob = xgb_model.predict(dtest)\n",
    "\n",
    "# Perform blending (simple average)\n",
    "final_test_predictions = np.argmax(\n",
    "    (0.5 * minilm_test_probabilities) + (0.5 * xgb_test_pred_prob), axis=1)\n",
    "\n",
    "# Decode the final blended predictions back to original labels\n",
    "final_test_labels = label_encoder.inverse_transform(final_test_predictions)\n",
    "\n",
    "# Create a final submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': final_test_labels\n",
    "})\n",
    "submission_df.to_csv('blended_submission.csv', index=False)\n",
    "\n",
    "print(\"Blending with XGBoost completed. Final predictions saved to 'blended_submission.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
