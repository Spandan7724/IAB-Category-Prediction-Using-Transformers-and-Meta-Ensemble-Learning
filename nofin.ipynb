{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  Word Count\n",
      "0  python courses python courses, python exercise...       0         125\n",
      "1  the learning point open digital education. a r...       0         147\n",
      "2  tech news, latest technology, mobiles, laptops...       0         143\n",
      "3  the best it certification materials in usa | k...       0         364\n",
      "4  bioland scientific, for your research needs bi...       0         176\n",
      "Classes: ['academic interests' 'arts and culture' 'automotives'\n",
      " 'books and literature' 'business and finance' 'careers'\n",
      " 'family and relationships' 'food and drinks' 'health' 'healthy living'\n",
      " 'hobbies and interests' 'home and garden' 'movies' 'music and audio'\n",
      " 'news and politics' 'personal finance' 'pets'\n",
      " 'pharmaceuticals, conditions, and symptoms' 'real estate' 'shopping'\n",
      " 'sports' 'style and fashion' 'technology and computing' 'television'\n",
      " 'travel' 'video gaming']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['text'] = train_df['text'].str.lower().str.strip()\n",
    "test_df['text'] = test_df['text'].str.lower().str.strip()\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['text'], train_df['target'], test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Check the results of preprocessing and encoding\n",
    "print(train_df.head()) \n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'lockdown perfect time tollywood stars introspect line up outside profession actor actress began drive back first love music shruti hasaan others like pranitha subhash turned help others tough times various opening samantha akkineni find out foretell horticulture growing bring about said last found something passionate break job starting get tired answering people asked hobby reply represent counterargument job hobby oh baby actress began journey first harvest cabbage microgreens lockdown carry insta explained one need declamatory lawn backyard gardening using space uncommitted domicile initially used window sill bedroom grow microgreens said interested growing require tray cocopeat seeds cool room used bedroom window lets sunlight partly also gave guide fans farsighted takes cum sprout number days tray needs covered use lamp case one enough sunlight room inspired tollywood diva take gardening first piazza pandemic fear able-bodied feed oneself cut back houses said often hear eat healthy grow healthy even simpler takes little time effort since needs us home stay safe think manage let grow together feed god forbid ever another lockdown ones turn tail store panic gradually love growing mature led origination growwithme take exception people share stories have growing fresh vegetables fruits home showtime things along dookudu actress nominated lakshmi manchu rakul preet singh lakshmi manchu girl nivi planted ejaculate leading brand home kit up daughter contain excitement curiosity learn organic fertilizer work mother felicitous join initiative said pandemic taught us great mass healthy living eating food nourishes eubstance feel extremely proud able turn rakul preet singh growing spinach amaranth coriander st. basil excited chance grow solid food said cover girl experience watch seeds pop grow food celebs lot samantha fan young took challenge started growing vegetables like carrots spinach petroselinum crispum basil lettuce tomatoes baby rocket baby bok choy cucumbers celery started taking smashing interest cooking quite insta stories majili actress seen learning basics cooking professional sridevi jasti also friend started basics like taalimpu smoothies chia pudding went create complete meals like tom yum soup tofu brown rice noodles amaranth curry green beans zucchini kale wrap said slowly starting realize much takes put one repast tollywood diva turned greens warrior stop encourages everyone eat clean fresh makes bio enzyme home compost corner twin drum composter reuse recycle neutralize even break tips use waste like banana peels effectively make compost', 'labels': 10, '__index_level_0__': 412155}\n",
      "{'text': 'trafford garden rooms at trafford garden rooms we have the perfect home solution. with our wide range of garden rooms, for all sizes of gardens, we bring to life your dreams for your home. call us today! trafford garden rooms luxury real estate for sale - property agents & brokers - uptown.com  uptown.com luxury real estate for sale ? property agents & brokers ? get access to exclusive properties for sale in nationwide.  get the best agent/broker to help you! realtors michael & anita marchena are the best temecula realtors. realtors michael & anita marchena are the best realtors in temecula california and can help you buy or sell. why not work with the best temecula realtors?', 'labels': 18, '__index_level_0__': 682961}\n",
      "{'text': 'equl offers enzyme assay kits, reagent mixtures, enzymes, glycobiology, amylase test, carbohydrase tablet tests, protease tablet tests, cofactors and stains, soluble chromogenic substrates, insoluble chromogenic substrates, etc brands including: 3m / a.g.scientific / advanced targeting system / advanced biomatrix / agdia / agilent / ampackapak / auvon / aveslab / avonchem / bachem-peninsula / bd / biosb / bioxcell / bioclone / c&b / cadence / californiapeptideresearch / capillarytubes / cbs / chemetrics / chromotek / clodrosome / dako / diatome / divbio / drummond scientific / dumont / dyesol / e&kscientific / ebpi / electronmicroscopy / elisa systems / emsdiasum / encapsulanano / excell / fhc / finesciencetools (fst) / finewire /frontier institute / fuller lab / gene-tools / genevabiotech / glascol / goldbio / harvard / hausser scientific / hawksley / iba / ibl / ideal-tek / iduron / inscopix / ira / isosep / ist / j-kem / kapak / kerafast / kinematica / king precision glass / lumafluor / magle / mattek / mediagnost / medkoo / megazyme / micromod / miltenyi / mybiosource / nacalai / neuro probe / nisco / optical imaging ltd / orbeco / ovenindustries / paperthermometer / parkell / peninsula laboratories / phadebas / phagoburst / plasticsone / pointe scientific / popper / prokazyme / qorpak / quantifoil / radiation alert / randox / roboz surgical instrument / saint-gobain / sakura / scientific instrument service / se / sekisui diagnostics / scientific industries (si) / sigma / sobioda / spectrapor / stanbio / sutter instrument / swant / synaptic system (sysy) / synergel / synthecon / ted pella / teknova / tissue-tek / toronto research chemicals / trinity biotech / v&p scientific / viagen / wako / willco wells bv / world precision instruments (wpi) / worthington biochemical instruments consumables reagents advanced biomatrix randox randox elisa biomedical  biochemical reagents  laboratory supplies  equipment  antibodies  elisa kits  diagnostic reagents  methods of experimental techniques  general analytical instruments  material testing instruments and equipment  used laboratory equipment  instruments and equipment  life sciences  environmental monitoring equipment   measurement  measuring instruments  rotating wall bioreactor  three-dimensional tissue / stem cell culture system; microcapsule'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert pandas DataFrames to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(\n",
    "    {'text': train_texts, 'labels': train_labels}))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame(\n",
    "    {'text': val_texts, 'labels': val_labels}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': test_df['text']}))\n",
    "\n",
    "# Display first few entries to verify datasets\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at PavanDeepak/text-classification-model-iab-categories-mixed-bert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([364]) in the checkpoint and torch.Size([26]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([364, 768]) in the checkpoint and torch.Size([26, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'PavanDeepak/text-classification-model-iab-categories-mixed-bert-base-uncased', use_fast=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'PavanDeepak/text-classification-model-iab-categories-mixed-bert-base-uncased',\n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    ignore_mismatched_sizes=True  # Ignore the size mismatch for the classifier layer\n",
    ")\n",
    "\n",
    "# Alternatively, if you want to replace the classifier layer with a new one\n",
    "# model.classifier = torch.nn.Linear(model.config.hidden_size, len(label_encoder.classes_))\n",
    "\n",
    "# Freeze all layers except the last 2 to 4 transformer layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for i in range(-4, 0):  # Modify this range to freeze fewer or more layers\n",
    "    for param in model.bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets loaded successfully from disk.\n",
      "{'labels': tensor(10), 'input_ids': tensor([  101,  5843,  7698,  3819,  2051,  9565, 26985,  3340, 17174, 13102,\n",
      "        22471,  2240,  2039,  2648,  9518,  3364,  3883,  2211,  3298,  2067,\n",
      "         2034,  2293,  2189, 14021, 22134,  2072,  2038, 14634,  2500,  2066,\n",
      "        10975,  7088,  8322,  4942, 14949,  2232,  2357,  2393,  2500,  7823,\n",
      "         2335,  2536,  3098, 11415, 17712,  4939, 18595,  2424,  2041, 18921,\n",
      "        23567,  7570, 28228, 14561,  3652,  3288,  2055,  2056,  2197,  2179,\n",
      "         2242, 13459,  3338,  3105,  3225,  2131,  5458, 10739,  2111,  2356,\n",
      "        17792,  7514,  5050,  4675,  2906, 22850,  4765,  3105, 17792,  2821,\n",
      "         3336,  3883,  2211,  4990,  2034, 11203, 28540, 12702, 28637,  3619,\n",
      "         5843,  7698,  4287, 16021,  2696,  4541,  2028,  2342, 11703, 10278,\n",
      "        14049, 10168, 16125, 21529,  2478,  2686,  4895,  9006, 22930,  3064,\n",
      "        14383, 28775,  2571,  3322,  2109,  3332,  9033,  3363,  5010,  4982,\n",
      "        12702, 28637,  3619,  2056,  4699,  3652,  5478, 11851, 25033,  5051,\n",
      "         4017,  8079,  4658,  2282,  2109,  5010,  3332, 11082,  9325,  6576,\n",
      "         2036,  2435,  5009,  4599,  2521, 25807,  2098,  3138, 13988, 11867,\n",
      "        22494,  2102,  2193,  2420, 11851,  3791,  3139,  2224, 10437,  2553,\n",
      "         2028,  2438,  9325,  2282,  4427,  9565, 26985, 25992,  2202, 21529,\n",
      "         2034, 22463,  6090,  3207,  7712,  3571,  2583,  1011, 22549,  5438,\n",
      "        25763,  3013,  2067,  3506,  2056,  2411,  2963,  4521,  7965,  4982,\n",
      "         7965,  2130, 16325,  3138,  2210,  2051,  3947,  2144,  3791,  2149,\n",
      "         2188,  2994,  3647,  2228,  6133,  2292,  4982,  2362,  5438,  2643,\n",
      "        27206,  2412,  2178,  5843,  7698,  3924,  2735,  5725,  3573,  6634,\n",
      "         6360,  2293,  3652,  9677,  2419,  4761,  3370,  4982, 24415,  4168,\n",
      "         2202,  6453,  2111,  3745,  3441,  2031,  3652,  4840, 11546, 10962,\n",
      "         2188, 23811,  2477,  2247, 20160,  5283,  8566,  3883,  4222, 21352,\n",
      "        26650, 10958,  5283,  2140,  3653,  3388,  5960, 21352, 26650,  2611,\n",
      "         9152,  5737,  8461,  1041,  3900, 19879,  2618,  2877,  4435,  2188,\n",
      "         8934,  2039,  2684,  5383,  8277, 10628,  4553,  7554, 10768, 28228,\n",
      "        28863,  2147,  2388, 10768, 10415,  9956,  2271,  3693,  6349,  2056,\n",
      "         6090,  3207,  7712,  4036,  2149,  2307,  3742,  7965,  2542,  5983,\n",
      "         2833,  2053,  9496,  4095,  2229,  7327,  5910, 26897,  2514,  5186,\n",
      "         7098,  2583,  2735, 10958,  5283,  2140,  3653,  3388,  5960,  3652,\n",
      "         6714,  6776, 28599,  3372,  2232,  2522,  6862,  4063,  2358,  1012,\n",
      "        14732,  7568,  3382,  4982,  5024,  2833,  2056,  3104,  2611,  3325,\n",
      "         3422,  8079,  3769,  4982,  2833,  8292,  2571,  5910,  2843, 11415,\n",
      "         5470,  2402,  2165,  4119,  2318,  3652, 11546,  2066, 25659,  2015,\n",
      "         6714,  6776,  9004, 13278,  4115,  2819, 15594,  2819, 14732,  2292,\n",
      "         8525,  3401, 12851,  3336,  7596,  3336,  8945,  2243, 16480,  2100,\n",
      "        12731, 24894, 17198,  8292,  3917,  2100,  2318,  2635, 21105,  3037,\n",
      "         8434,  3243, 16021,  2696,  3441, 16686, 18622,  3883,  2464,  4083,\n",
      "        24078,  8434,  2658,  5185, 24844,  2072, 14855, 16643,  2036,  2767,\n",
      "         2318, 24078,  2066, 11937, 11475,  8737,  2226,  5744,  3111,  9610,\n",
      "         2050, 29593,  2253,  3443,  3143, 12278,  2066,  3419,  9805,  2213,\n",
      "        11350,  2000, 11263,  2829,  5785, 27130, 28599,  3372,  2232, 15478,\n",
      "         2665, 13435, 16950, 25955,  3490, 10556,  2571, 10236,  2056,  3254,\n",
      "         3225,  5382,  2172,  3138,  2404,  2028, 16360, 14083,  9565, 26985,\n",
      "        25992,  2357, 15505,  6750,  2644, 16171,  3071,  4521,  4550,  4840,\n",
      "         3084, 16012,  9007,  2188,  4012, 19894,  3420,  5519,  6943,  4012,\n",
      "        19894,  2121,  2128,  8557, 28667,  2100, 14321,  8699,  4697,  2130,\n",
      "         3338, 10247,  2224,  5949,  2066, 15212, 14113,  2015,  6464,  2191,\n",
      "         4012, 19894,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "{'labels': tensor(18), 'input_ids': tensor([  101, 26894,  3871,  4734,  2012, 26894,  3871,  4734,  2057,  2031,\n",
      "         1996,  3819,  2188,  5576,  1012,  2007,  2256,  2898,  2846,  1997,\n",
      "         3871,  4734,  1010,  2005,  2035, 10826,  1997,  5822,  1010,  2057,\n",
      "         3288,  2000,  2166,  2115,  5544,  2005,  2115,  2188,  1012,  2655,\n",
      "         2149,  2651,   999, 26894,  3871,  4734,  9542,  2613,  3776,  2005,\n",
      "         5096,  1011,  3200,  6074,  1004, 20138,  2015,  1011, 28539,  1012,\n",
      "         4012, 28539,  1012,  4012,  9542,  2613,  3776,  2005,  5096,  1029,\n",
      "         3200,  6074,  1004, 20138,  2015,  1029,  2131,  3229,  2000,  7262,\n",
      "         5144,  2005,  5096,  1999,  9053,  1012,  2131,  1996,  2190,  4005,\n",
      "         1013, 20138,  2000,  2393,  2017,   999,  2613,  6591,  2745,  1004,\n",
      "        12918, 28791,  2532,  2024,  1996,  2190,  8915,  4168, 19879,  2613,\n",
      "         6591,  1012,  2613,  6591,  2745,  1004, 12918, 28791,  2532,  2024,\n",
      "         1996,  2190,  2613,  6591,  1999,  8915,  4168, 19879,  2662,  1998,\n",
      "         2064,  2393,  2017,  4965,  2030,  5271,  1012,  2339,  2025,  2147,\n",
      "         2007,  1996,  2190,  8915,  4168, 19879,  2613,  6591,  1029,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "# Initialize the tokenizer\n",
    "\n",
    "\n",
    "train_dataset_path = 'nofin/tokenized_train_dataset'\n",
    "val_dataset_path = 'nofin/tokenized_val_dataset'\n",
    "test_dataset_path = 'nofin/tokenized_test_dataset'\n",
    "\n",
    "if os.path.exists(train_dataset_path) and os.path.exists(val_dataset_path) and os.path.exists(test_dataset_path):\n",
    "    # Load the tokenized datasets\n",
    "    train_dataset = load_from_disk(train_dataset_path)\n",
    "    val_dataset = load_from_disk(val_dataset_path)\n",
    "    test_dataset = load_from_disk(test_dataset_path)\n",
    "    print(\"Tokenized datasets loaded successfully from disk.\")\n",
    "else:\n",
    "    # Define the tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Perform tokenization without multiprocessing\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Set the format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=[\n",
    "                             'input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format(type='torch', columns=[\n",
    "                           'input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=[\n",
    "                            'input_ids', 'attention_mask'])\n",
    "\n",
    "    # Save the tokenized datasets to disk\n",
    "    os.makedirs('nofin', exist_ok=True)\n",
    "    train_dataset.save_to_disk(train_dataset_path)\n",
    "    val_dataset.save_to_disk(val_dataset_path)\n",
    "    test_dataset.save_to_disk(test_dataset_path)\n",
    "    print(\"Tokenized datasets saved successfully to disk.\")\n",
    "\n",
    "# Check the results of tokenization\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=26, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    dataloader_num_workers=8,\n",
    "    output_dir='./nofin/results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='none',\n",
    "    logging_steps=5000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "# Display model details\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer.Trainer object at 0x0000027C23895370>\n"
     ]
    }
   ],
   "source": [
    "# Define a function for computing metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    return {'f1': f1, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "# Verify trainer configuration\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should output True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc3c1b8f5d1495a862e25ad7f462aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.5878, 'train_samples_per_second': 3203897.375, 'train_steps_per_second': 25027.976, 'train_loss': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14712, training_loss=0.0, metrics={'train_runtime': 0.5878, 'train_samples_per_second': 3203897.375, 'train_steps_per_second': 25027.976, 'total_flos': 4.9557961552082534e+17, 'train_loss': 0.0, 'epoch': 2.999898052808645})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint='./nofin/results/checkpoint-14712')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c6f3675c5f41aba09b4a2310943137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'final_submission.csv' generated successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd41cb37d2b849bd8cfe625533283093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Decode the predictions back to original labels\n",
    "pred_labels_decoded = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# Create the submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': pred_labels_decoded\n",
    "})\n",
    "submission_df.to_csv('nofine_submission.csv', index=False)\n",
    "print(\"Submission file 'final_submission.csv' generated successfully.\")\n",
    "\n",
    "nofin_predictions = trainer.predict(test_dataset)\n",
    "# Save predicted probabilities for later ensemble use\n",
    "nofin_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "np.save('nofine_model_probabilities.npy', nofin_probabilities)\n",
    "\n",
    "nofin_pred_labels = np.argmax(nofin_probabilities, axis=1)\n",
    "np.save('nofin_pred_labels.npy', nofin_pred_labels)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('nofine_model')\n",
    "tokenizer.save_pretrained('nofine_tokenizer')\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c163508b135474282ca001c5e1e60c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the training set for nofine\n",
    "nofine_train_predictions = trainer.predict(train_dataset)\n",
    "\n",
    "# Save predicted probabilities for the training set\n",
    "nofine_train_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(nofine_train_predictions.predictions), dim=-1).numpy()\n",
    "np.save('nofine_train_probabilities.npy', nofine_train_probabilities)\n",
    "\n",
    "# Save the actual predicted labels for the training set\n",
    "nofine_train_pred_labels = np.argmax(nofine_train_probabilities, axis=1)\n",
    "np.save('nofine_train_pred_labels.npy', nofine_train_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223e90244eb24d7dafd648c28952745f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 123\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy}\n\u001b[0;32m    112\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    113\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    114\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)]\n\u001b[0;32m    120\u001b[0m )\n\u001b[1;32m--> 123\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[0;32m    127\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91898\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\trainer.py:2281\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "# Load the previously saved model and tokenizer\n",
    "model_path = 'nofine_model'\n",
    "tokenizer_path = 'nofine_tokenizer'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for i in range(-6, -4):  \n",
    "    for param in model.bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/train.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(\n",
    "    'C:/Users/91898/Code/fibe/dataset_fibe/test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Preprocess the text data\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(\n",
    "    train_df[['text', 'target']].rename(columns={'target': 'labels'}))\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']])\n",
    "\n",
    "# Load previously tokenized datasets if they exist or perform tokenization again\n",
    "train_dataset_path = 'nofin/tokenized_train_dataset'\n",
    "val_dataset_path = 'nofin/tokenized_val_dataset'\n",
    "test_dataset_path = 'nofin/tokenized_test_dataset'\n",
    "\n",
    "if os.path.exists(train_dataset_path) and os.path.exists(val_dataset_path) and os.path.exists(test_dataset_path):\n",
    "    train_dataset = Dataset.load_from_disk(train_dataset_path)\n",
    "    val_dataset = Dataset.load_from_disk(val_dataset_path)\n",
    "    test_dataset = Dataset.load_from_disk(test_dataset_path)\n",
    "else:\n",
    "    # Define the tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Perform tokenization without multiprocessing\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Set the format for PyTorch\n",
    "    train_dataset.set_format(type='torch', columns=[\n",
    "                             'input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format(type='torch', columns=[\n",
    "                            'input_ids', 'attention_mask'])\n",
    "\n",
    "    # Save the tokenized datasets to disk\n",
    "    os.makedirs('nofin', exist_ok=True)\n",
    "    train_dataset.save_to_disk(train_dataset_path)\n",
    "    test_dataset.save_to_disk(test_dataset_path)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./nofin/results',\n",
    "    num_train_epochs=3,  \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='none',\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  \n",
    "    report_to=\"none\",\n",
    "    resume_from_checkpoint=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = metrics.accuracy_score(labels, predictions)\n",
    "    return {'f1': f1, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "pred_labels_decoded = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Index': 'Article_' + test_df.index.astype(str),\n",
    "    'target': pred_labels_decoded\n",
    "})\n",
    "submission_df.to_csv('6layer_submission.csv', index=False)\n",
    "print(\"Submission file '6layer_submission.csv' generated successfully.\")\n",
    "\n",
    "\n",
    "nofin_probabilities = torch.nn.functional.softmax(\n",
    "    torch.tensor(predictions.predictions), dim=-1).numpy()\n",
    "np.save('nofine_model_probabilities1.npy', nofin_probabilities)\n",
    "\n",
    "nofin_pred_labels = np.argmax(nofin_probabilities, axis=1)\n",
    "np.save('nofin_pred_labels1.npy', nofin_pred_labels)\n",
    "\n",
    "model.save_pretrained('nofine_model_updated')\n",
    "tokenizer.save_pretrained('nofine_tokenizer_updated')\n",
    "print(\"Model and tokenizer with 6 layers unfrozen saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
